{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMGgFt/D1Mof1atucaVoQ1T"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install tiktoken"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M8EYXFZJecHH","executionInfo":{"status":"ok","timestamp":1742710692006,"user_tz":-330,"elapsed":5698,"user":{"displayName":"Ankit Kashyap","userId":"08351659539632815056"}},"outputId":"66c9cfef-4757-4d7b-e220-426227ae97ca"},"execution_count":120,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n"]}]},{"cell_type":"code","source":["import tiktoken\n","import torch\n","\n","\n","class TokenizationLayer:\n","    def __init__(self, model_name=\"cl100k_base\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n","        \"\"\"\n","        Tokenization Layer using tiktoken.\n","\n","        Args:\n","            model_name (str): Name of the tokenizer model. Default is 'cl100k_base'.\n","            device (str): Device to run on ('cuda' or 'cpu').\n","        \"\"\"\n","        self.device = torch.device(device)\n","\n","        # Load tokenizer\n","        self.tokenizer = tiktoken.get_encoding(model_name)\n","        self.vocab_size = self.tokenizer.n_vocab\n","\n","        # Define custom tokens (CLS and SEP)\n","        self.cls_token = \"<|cls|>\"\n","        self.sep_token = \"<|sep|>\"\n","\n","        # Manually assign token IDs for custom tokens\n","        try:\n","            self.cls_token_id = self.tokenizer.encode(self.cls_token)[0]\n","        except KeyError:\n","            print(f\"Warning: {self.cls_token} not found in tokenizer. Using eot_token instead.\")\n","            self.cls_token_id = self.tokenizer.eot_token\n","\n","        try:\n","            self.sep_token_id = self.tokenizer.encode(self.sep_token)[0]\n","        except KeyError:\n","            print(f\"Warning: {self.sep_token} not found in tokenizer. Using eot_token instead.\")\n","            self.sep_token_id = self.tokenizer.eot_token\n","\n","        # Use pad_token if available, else use eot_token\n","        self.pad_token_id = self.tokenizer.eot_token  # Default to EOT token\n","        if hasattr(self.tokenizer, \"pad_token\"):\n","            self.pad_token_id = self.tokenizer.pad_token\n","\n","    def tokenize(self, texts, max_length=512, add_special_tokens=True):\n","        \"\"\"\n","        Tokenizes input texts into token IDs with optional padding, truncation, and special tokens.\n","\n","        Args:\n","            texts (str or List[str]): Input text or list of texts to be tokenized.\n","            max_length (int): Maximum sequence length. Default is 512.\n","            add_special_tokens (bool): Whether to add special tokens (CLS, SEP). Default is True.\n","\n","        Returns:\n","            torch.Tensor: Token IDs with shape [batch_size, max_length] on the specified device.\n","        \"\"\"\n","        if isinstance(texts, str):  # Handle single text input\n","            texts = [texts]\n","\n","        token_ids = []\n","        for text in texts:\n","            # Tokenize text\n","            tokens = self.tokenizer.encode(text)\n","\n","            tokens = [min(token, self.vocab_size - 1) for token in tokens]\n","\n","            # Add special tokens (CLS and SEP)\n","            if add_special_tokens:\n","                tokens = [self.cls_token_id] + tokens + [self.sep_token_id]\n","\n","            # Truncate if necessary\n","            tokens = tokens[:max_length]\n","\n","            # Pad if necessary\n","            if len(tokens) < max_length:\n","                tokens += [self.pad_token_id] * (max_length - len(tokens))\n","\n","            token_ids.append(tokens)\n","\n","        return torch.tensor(token_ids, dtype=torch.long, device=self.device)  # Send to GPU\n","\n","    def detokenize(self, tokens):\n","        \"\"\"\n","        Converts token IDs back to text.\n","\n","        Args:\n","            tokens (List[int] or torch.Tensor): Tokenized input.\n","\n","        Returns:\n","            str or List[str]: Decoded text or list of texts.\n","        \"\"\"\n","        if isinstance(tokens, torch.Tensor):\n","            tokens = tokens.cpu().tolist()  # Convert to list and move to CPU if tensor\n","\n","        if isinstance(tokens[0], list):  # Handle batch input\n","            return [self.tokenizer.decode(t) for t in tokens]\n","        else:  # Handle single input\n","            return self.tokenizer.decode(tokens)\n","\n","\n","# ✅ CUDA Check\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using Device:\", device)\n","\n","# ✅ cuDNN Acceleration Check\n","if torch.backends.cudnn.is_available():\n","    print(\"cuDNN Enabled:\", torch.backends.cudnn.enabled)\n","\n","# ✅ Testing Tokenization Layer\n","tokenizer_layer = TokenizationLayer(device=device)\n","\n","# Single Text Example\n","text = \"Bhai, hum Layer 1 ka Tokenization implement kar rahe hain!\"\n","tokens = tokenizer_layer.tokenize(text, max_length=10, add_special_tokens=True)  # Shape: [1, max_length]\n","decoded_text = tokenizer_layer.detokenize(tokens)\n","\n","# Batch Text Example\n","texts = [\n","    \"Bhai, hum Layer 1 ka Tokenization implement kar rahe hain!\",\n","    \"Ye code ekdam perfect hai!\"\n","]\n","batch_tokens = tokenizer_layer.tokenize(texts, max_length=10, add_special_tokens=True)  # Shape: [batch_size, max_length]\n","decoded_texts = tokenizer_layer.detokenize(batch_tokens)\n","\n","# ✅ Outputs\n","print(\"\\nSingle Text Example:\")\n","print(\"Input Text:\", text)\n","print(\"Tokenized Output (Shape: {}):\".format(tokens.shape), tokens)\n","print(\"Decoded Text:\", decoded_text)\n","\n","print(\"\\nBatch Text Example:\")\n","print(\"Input Texts:\", texts)\n","print(\"Tokenized Output (Shape: {}):\".format(batch_tokens.shape), batch_tokens)\n","print(\"Decoded Texts:\", decoded_texts)\n"],"metadata":{"id":"im-i2LnvQ2X8","executionInfo":{"status":"ok","timestamp":1742710692044,"user_tz":-330,"elapsed":22,"user":{"displayName":"Ankit Kashyap","userId":"08351659539632815056"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c955d263-992b-489a-da43-7a38e4ca1556"},"execution_count":121,"outputs":[{"output_type":"stream","name":"stdout","text":["Using Device: cpu\n","cuDNN Enabled: True\n","\n","Single Text Example:\n","Input Text: Bhai, hum Layer 1 ka Tokenization implement kar rahe hain!\n","Tokenized Output (Shape: torch.Size([1, 10])): tensor([[   27,    33, 26279,    11,  2854, 23570,   220,    16, 16909,  9857]])\n","Decoded Text: ['<Bhai, hum Layer 1 ka Token']\n","\n","Batch Text Example:\n","Input Texts: ['Bhai, hum Layer 1 ka Tokenization implement kar rahe hain!', 'Ye code ekdam perfect hai!']\n","Tokenized Output (Shape: torch.Size([2, 10])): tensor([[    27,     33,  26279,     11,   2854,  23570,    220,     16,  16909,\n","           9857],\n","        [    27,  87575,   2082,  27955,  15770,   4832,  47151,      0,     27,\n","         100257]])\n","Decoded Texts: ['<Bhai, hum Layer 1 ka Token', '<Ye code ekdam perfect hai!<<|endoftext|>']\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.amp import autocast\n","# from TokenizationLayer import TokenizationLayer\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","class TokenEmbedding(nn.Module):\n","    def __init__(self, vocab_size, embed_dim, padding_idx=0, dropout=0.1):\n","        \"\"\"\n","        Token Embedding Layer using PyTorch nn.Embedding.\n","\n","        Args:\n","            vocab_size (int): Number of unique tokens in vocabulary.\n","            embed_dim (int): Dimension of each token embedding.\n","            padding_idx (int, optional): Index of padding token. Default: 0.\n","            dropout (float, optional): Dropout probability. Default: 0.1.\n","        \"\"\"\n","        super(TokenEmbedding, self).__init__()\n","        self.embedding = nn.Embedding(\n","            num_embeddings=vocab_size,\n","            embedding_dim=embed_dim,\n","            padding_idx=padding_idx  # Helps handle padding tokens efficiently\n","        )\n","        self.dropout = nn.Dropout(dropout)  # Dropout for regularization\n","        nn.init.xavier_uniform_(self.embedding.weight)  # Better initialization\n","\n","    def forward(self, input_tokens):\n","        \"\"\"\n","        Forward pass to convert token IDs to embeddings.\n","\n","        Args:\n","            input_tokens (torch.Tensor): Tensor of shape (batch_size, seq_len).\n","\n","        Returns:\n","            torch.Tensor: Token embeddings of shape (batch_size, seq_len, embed_dim).\n","        \"\"\"\n","        # Ensure input is on the correct device\n","        input_tokens = input_tokens.to(self.embedding.weight.device)\n","\n","        # Apply embedding and dropout\n","\n","        embeddings = self.embedding(input_tokens)\n","        embeddings = self.dropout(embeddings)  # Apply dropout\n","\n","        return embeddings\n","\n","\n","# ✅ Hyperparameters\n","vocab_size = 10000  # Size of vocabulary\n","embed_dim = 512  # Embedding dimension per token\n","batch_size = 8  # Number of sequences processed in parallel\n","seq_len = 128  # Max sequence length\n","\n","\n","\n","tokenizer_layer = TokenizationLayer()\n","# ✅ Initialize Token Embedding Layer\n","# ✅ Initialize Token Embedding Layer with correct vocab size\n","token_embedding = TokenEmbedding(\n","    vocab_size=tokenizer_layer.vocab_size,  # Use actual vocab size\n","    embed_dim=512\n",").to(device)\n","\n","# ✅ Example Input (Random Token IDs)\n","input_tokens = torch.randint(0, vocab_size, (batch_size, seq_len), device=device, dtype=torch.long)\n","\n","# ✅ Apply Token Embedding\n","output_embeddings = token_embedding(input_tokens)\n","\n","# ✅ Debugging Info\n","print(\"\\n✅ Input Tokens Shape:\", input_tokens.shape)  # Expected: (8, 128)\n","print(\"✅ Token Embedding Output Shape:\", output_embeddings.shape)  # Expected: (8, 128, 512)\n","print(\"✅ Token Embedding Output dtype:\", output_embeddings.dtype)  # Should be float16 if AMP enabled\n","\n","\n"],"metadata":{"id":"GSxAZKllQ2WD","executionInfo":{"status":"ok","timestamp":1742710693150,"user_tz":-330,"elapsed":1105,"user":{"displayName":"Ankit Kashyap","userId":"08351659539632815056"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"857707c2-f02b-49cb-b350-98b5240704a4"},"execution_count":122,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","✅ Input Tokens Shape: torch.Size([8, 128])\n","✅ Token Embedding Output Shape: torch.Size([8, 128, 512])\n","✅ Token Embedding Output dtype: torch.float32\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","\n","\n","\n","class RotaryPositionalEncoding(nn.Module):\n","    def __init__(self, embed_dim):\n","        \"\"\"\n","        Rotary Positional Encoding (RoPE) for transformers.\n","\n","        Args:\n","            embed_dim (int): Dimension of token embeddings.\n","        \"\"\"\n","        super(RotaryPositionalEncoding, self).__init__()\n","        self.embed_dim = embed_dim\n","\n","        # Compute inverse frequency terms for RoPE\n","        inv_freq = 1.0 / (10000 ** (torch.arange(0, embed_dim, 2, dtype=torch.float32) / embed_dim))\n","        self.register_buffer(\"inv_freq\", inv_freq)  # Store as buffer\n","\n","    def rotate_half(self, x):\n","        \"\"\"\n","        Rotates the last dimension by 90 degrees.\n","\n","        Args:\n","            x (torch.Tensor): Input tensor of shape (..., embed_dim).\n","\n","        Returns:\n","            torch.Tensor: Rotated tensor of same shape.\n","        \"\"\"\n","        x1, x2 = x.chunk(2, dim=-1)\n","        return torch.cat((-x2, x1), dim=-1)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass for RoPE.\n","\n","        Args:\n","            x (torch.Tensor): Token embeddings of shape (batch_size, seq_len, embed_dim).\n","\n","        Returns:\n","            torch.Tensor: Rotated embeddings with positional information.\n","        \"\"\"\n","        batch_size, seq_len, embed_dim = x.shape\n","\n","        # Generate position indices\n","        positions = torch.arange(seq_len, dtype=torch.float32, device=x.device).unsqueeze(1)\n","\n","        # Compute rotation frequencies\n","        freqs = torch.matmul(positions, self.inv_freq.unsqueeze(0))  # Shape: [seq_len, embed_dim//2]\n","        emb = torch.cat((freqs, freqs), dim=-1)  # Shape: [seq_len, embed_dim]\n","\n","        # Compute cos and sin embeddings\n","        cos_emb, sin_emb = emb.cos().unsqueeze(0), emb.sin().unsqueeze(0)  # Shape: [1, seq_len, embed_dim]\n","\n","        # Apply RoPE transformation\n","        x_rotated = (x * cos_emb) + (self.rotate_half(x) * sin_emb)\n","\n","        return x_rotated\n","\n","\n","# ✅ Hyperparameters\n","batch_size = 8\n","seq_len = 128\n","embed_dim = 512\n","\n","# ✅ Initialize RoPE\n","rotary_pe = RotaryPositionalEncoding(embed_dim).to(device)\n","\n","# ✅ Example Input (Random Token Embeddings)\n","input_embeddings = torch.randn(batch_size, seq_len, embed_dim, dtype=torch.float32, device=device)\n","\n","# ✅ Apply RoPE\n","output_embeddings = rotary_pe(input_embeddings)\n","\n","# ✅ Debugging Info\n","print(\"✅ Input Embeddings Shape:\", input_embeddings.shape)  # Expected: (8, 128, 512)\n","print(\"✅ RoPE Output Shape:\", output_embeddings.shape)  # Expected: (8, 128, 512)\n","print(\"✅ RoPE Output dtype:\", output_embeddings.dtype)  # Expected: float32"],"metadata":{"id":"x_0g74WEQ2R7","executionInfo":{"status":"ok","timestamp":1742710693224,"user_tz":-330,"elapsed":73,"user":{"displayName":"Ankit Kashyap","userId":"08351659539632815056"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"12f65ede-ba76-4c73-adcf-8de6a053a426"},"execution_count":123,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Input Embeddings Shape: torch.Size([8, 128, 512])\n","✅ RoPE Output Shape: torch.Size([8, 128, 512])\n","✅ RoPE Output dtype: torch.float32\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","\n","\n","# ✅ Optimized Layer Normalization (Pre-LN)\n","# Define LayerNorm (if not already defined)\n","class LayerNorm(nn.Module):\n","    def __init__(self, embed_dim, eps=1e-5):\n","        super(LayerNorm, self).__init__()\n","        self.gamma = nn.Parameter(torch.ones(embed_dim, dtype=torch.float32))  # Learnable scale\n","        self.beta = nn.Parameter(torch.zeros(embed_dim, dtype=torch.float32))  # Learnable shift\n","        self.eps = eps  # Small value for numerical stability\n","\n","    def forward(self, x):\n","        # Apply Layer Normalization\n","        return F.layer_norm(x, x.shape[-1:], self.gamma, self.beta, self.eps)\n","\n","# Define ResidualConnection\n","class ResidualConnection(nn.Module):\n","    def __init__(self, embed_dim, dropout=0.1):\n","        super(ResidualConnection, self).__init__()\n","        self.norm = LayerNorm(embed_dim)  # Pre-LayerNorm\n","        self.dropout = nn.Dropout(dropout)  # Regularization\n","\n","    def forward(self, x, sublayer):\n","        # Apply Pre-LayerNorm, sublayer, and residual connection\n","        return x + self.dropout(sublayer(self.norm(x)))\n","\n","# Define MultiHeadSelfAttention (if not already defined)\n","class MultiHeadSelfAttention(nn.Module):\n","    def __init__(self, embed_dim, num_heads):\n","        super(MultiHeadSelfAttention, self).__init__()\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads  # Ensure num_heads is set as an attribute\n","        self.head_dim = embed_dim // num_heads\n","\n","        assert self.head_dim * num_heads == embed_dim, \"Embedding dimension must be divisible by number of heads\"\n","\n","        # Fused QKV Projection (Single Linear Layer for Efficiency)\n","        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim, dtype=torch.float32)\n","        self.out_proj = nn.Linear(embed_dim, embed_dim, dtype=torch.float32)\n","\n","    def forward(self, x, mask=None):\n","        batch_size, seq_len, embed_dim = x.shape\n","\n","        # Compute Q, K, V in a single pass\n","        qkv = self.qkv_proj(x).view(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n","        Q, K, V = qkv.unbind(dim=2)  # Split into separate tensors\n","\n","        # Reshape for multi-head attention\n","        Q = Q.transpose(1, 2)  # Shape: (batch, num_heads, seq_len, head_dim)\n","        K = K.transpose(1, 2)\n","        V = V.transpose(1, 2)\n","\n","        # Flash Attention (Optimized Scaled Dot-Product Attention)\n","        if mask is not None:\n","            mask = mask.to(dtype=torch.float16, device=x.device)  # Ensure mask is on the correct device and dtype\n","        output = F.scaled_dot_product_attention(Q, K, V, attn_mask=mask)\n","\n","        # Reshape back to original shape\n","        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n","\n","        # Apply output projection\n","        return self.out_proj(output)\n","\n","\n","\n","# ✅ Hyperparameters\n","batch_size = 8\n","seq_len = 128\n","embed_dim = 512\n","num_heads = 8\n","\n","# ✅ Initialize Layers\n","self_attention = MultiHeadSelfAttention(embed_dim, num_heads).to(device)\n","residual_connection = ResidualConnection(embed_dim).to(device)\n","\n","# ✅ Example Input (Random Token Embeddings)\n","input_embeddings = torch.randn(batch_size, seq_len, embed_dim, dtype=torch.float32, device=device)\n","\n","# ✅ Causal Mask (For Decoder)\n","mask = torch.tril(torch.ones(seq_len, seq_len, dtype=torch.float32, device=device)).unsqueeze(0).unsqueeze(0)\n","\n","# ✅ Apply Masked Multi-Head Self-Attention with Residual Connection\n","output_embeddings = residual_connection(input_embeddings, self_attention)\n","\n","# ✅ Debugging Info\n","print(\"✅ Input Embeddings Shape:\", input_embeddings.shape)  # Expected: (8, 128, 512)\n","print(\"✅ Mask Shape:\", mask.shape)  # Expected: (1, 1, 128, 128)\n","print(\"✅ Self-Attention Output Shape:\", output_embeddings.shape)  # Expected: (8, 128, 512)\n","print(\"✅ Output dtype:\", output_embeddings.dtype)  # Expected: float32"],"metadata":{"id":"mCe32WcmQ2QX","executionInfo":{"status":"ok","timestamp":1742710693357,"user_tz":-330,"elapsed":131,"user":{"displayName":"Ankit Kashyap","userId":"08351659539632815056"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ac531c91-f972-446f-9d17-c445362cfc96"},"execution_count":124,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Input Embeddings Shape: torch.Size([8, 128, 512])\n","✅ Mask Shape: torch.Size([1, 1, 128, 128])\n","✅ Self-Attention Output Shape: torch.Size([8, 128, 512])\n","✅ Output dtype: torch.float32\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","#from MultiHeadSelfAttention import MultiHeadSelfAttention\n","\n","\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# ✅ Optimized Layer Normalization (Pre-LN)\n","class LayerNorm(nn.Module):\n","    def __init__(self, embed_dim, eps=1e-5):\n","        super(LayerNorm, self).__init__()\n","        self.gamma = nn.Parameter(torch.ones(embed_dim, dtype=torch.float32))  # Learnable scale\n","        self.beta = nn.Parameter(torch.zeros(embed_dim, dtype=torch.float32))  # Learnable shift\n","        self.eps = eps  # Small value for numerical stability\n","\n","    def forward(self, x):\n","        # Apply Layer Normalization\n","        return F.layer_norm(x, x.shape[-1:], self.gamma, self.beta, self.eps)\n","\n","# ✅ Optimized Feedforward Network (FFN)\n","class FeedforwardNetwork(nn.Module):\n","    def __init__(self, embed_dim, hidden_dim, dropout=0.1):\n","        \"\"\"\n","        Feedforward Network with GELU Activation and Dropout.\n","        \"\"\"\n","        super(FeedforwardNetwork, self).__init__()\n","        self.linear1 = nn.Linear(embed_dim, hidden_dim, dtype=torch.float32)  # Expansion\n","        self.linear2 = nn.Linear(hidden_dim, embed_dim, dtype=torch.float32)  # Compression\n","        self.dropout = nn.Dropout(dropout)  # Regularization\n","        self.activation = nn.GELU()  # Activation function\n","\n","    def forward(self, x):\n","        x = self.linear1(x)  # Expand dimensions\n","        x = self.activation(x)  # Apply GELU\n","        x = self.dropout(x)  # Apply dropout\n","        x = self.linear2(x)  # Compress dimensions\n","        return x\n","\n","# ✅ Optimized Residual Connection with Pre-Norm\n","class ResidualConnection(nn.Module):\n","    def __init__(self, embed_dim, dropout=0.1):\n","        \"\"\"\n","        Residual Connection with Pre-Norm (Better Stability).\n","        \"\"\"\n","        super(ResidualConnection, self).__init__()\n","        self.norm = LayerNorm(embed_dim)  # Pre-LayerNorm\n","        self.dropout = nn.Dropout(dropout)  # Regularization\n","\n","    def forward(self, x, sublayer):\n","        # Apply Pre-LayerNorm, sublayer, and residual connection\n","        return x + self.dropout(sublayer(self.norm(x)))\n","\n","\n","\n","# ✅ Hyperparameters\n","batch_size = 8\n","seq_len = 128\n","embed_dim = 512\n","hidden_dim = 2048  # FFN hidden dimension\n","num_heads = 8\n","\n","# ✅ Initialize Layers\n","self_attention = MultiHeadSelfAttention(embed_dim, num_heads).to(device)\n","ffn = FeedforwardNetwork(embed_dim, hidden_dim).to(device)\n","residual_connection1 = ResidualConnection(embed_dim).to(device)\n","residual_connection2 = ResidualConnection(embed_dim).to(device)\n","\n","# ✅ Example Input (Random Token Embeddings)\n","input_embeddings = torch.randn(batch_size, seq_len, embed_dim, dtype=torch.float32, device=device)\n","\n","# ✅ Causal Mask (For Decoder)\n","mask = torch.tril(torch.ones(seq_len, seq_len, dtype=torch.float32, device=device)).unsqueeze(0).unsqueeze(0)\n","\n","# ✅ Apply Masked Multi-Head Self-Attention with Residual Connection\n","attention_output = residual_connection1(input_embeddings, self_attention)\n","\n","# ✅ Apply Feedforward Network with Residual Connection\n","ffn_output = residual_connection2(attention_output, ffn)\n","\n","# ✅ Debugging Info\n","print(\"✅ Input Embeddings Shape:\", input_embeddings.shape)  # Expected: (8, 128, 512)\n","print(\"✅ Attention Output Shape:\", attention_output.shape)  # Expected: (8, 128, 512)\n","print(\"✅ FFN Output Shape:\", ffn_output.shape)  # Expected: (8, 128, 512)\n","print(\"✅ Output dtype:\", ffn_output.dtype)  # Expected: float32"],"metadata":{"id":"cFH1wru4Q2Od","executionInfo":{"status":"ok","timestamp":1742710693592,"user_tz":-330,"elapsed":234,"user":{"displayName":"Ankit Kashyap","userId":"08351659539632815056"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0d8c636d-f2a2-4298-c0cc-87e166cb214a"},"execution_count":125,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Input Embeddings Shape: torch.Size([8, 128, 512])\n","✅ Attention Output Shape: torch.Size([8, 128, 512])\n","✅ FFN Output Shape: torch.Size([8, 128, 512])\n","✅ Output dtype: torch.float32\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","#from FeedforwardNetwork import LayerNorm,MultiHeadSelfAttention,ResidualConnection,FeedforwardNetwork\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","\n","\n","\n","\n","# ✅ Layer 1: Sublayers\n","class Layer1(nn.Module):\n","    def __init__(self, embed_dim, hidden_dim, num_heads, dropout=0.1):\n","        super(Layer1, self).__init__()\n","        # Sublayers\n","        self.pre_layer_norm = LayerNorm(embed_dim)  # Pre-LayerNorm\n","        self.self_attention = MultiHeadSelfAttention(embed_dim, num_heads)  # Self-Attention\n","        self.residual1 = ResidualConnection(embed_dim, dropout)  # Residual Connection 1\n","        self.layer_norm1 = LayerNorm(embed_dim)  # LayerNorm after Self-Attention\n","        self.ffn = FeedforwardNetwork(embed_dim, hidden_dim, dropout)  # Feedforward Network\n","        self.residual2 = ResidualConnection(embed_dim, dropout)  # Residual Connection 2\n","        self.layer_norm2 = LayerNorm(embed_dim)  # LayerNorm after FFN\n","\n","    def forward(self, x):\n","        # Sublayer 1: Self-Attention with Residual Connection\n","        x = self.residual1(x, self.self_attention)  # Self-Attention + Residual\n","        x = self.layer_norm1(x)  # LayerNorm after Self-Attention\n","\n","        # Sublayer 2: Feedforward Network with Residual Connection\n","        x = self.residual2(x, self.ffn)  # FFN + Residual\n","        x = self.layer_norm2(x)  # LayerNorm after FFN\n","\n","        return x\n","\n","\n","# ✅ Hyperparameters\n","batch_size = 8\n","seq_len = 128\n","embed_dim = 512\n","hidden_dim = 2048  # FFN hidden dimension\n","num_heads = 8\n","\n","# ✅ Initialize Layer 1\n","layer1 = Layer1(embed_dim, hidden_dim, num_heads).to(device)\n","\n","# ✅ Example Input (Random Token Embeddings)\n","input_embeddings = torch.randn(batch_size, seq_len, embed_dim, dtype=torch.float32, device=device)\n","\n","# ✅ Apply Layer 1\n","output = layer1(input_embeddings)\n","\n","# ✅ Debugging Info\n","print(\"✅ Input Embeddings Shape:\", input_embeddings.shape)  # Expected: (8, 128, 512)\n","print(\"✅ Layer 1 Output Shape:\", output.shape)  # Expected: (8, 128, 512)\n","print(\"✅ Output dtype:\", output.dtype)  # Expected: float32"],"metadata":{"id":"FNcPZ4I2Q2NG","executionInfo":{"status":"ok","timestamp":1742710693906,"user_tz":-330,"elapsed":283,"user":{"displayName":"Ankit Kashyap","userId":"08351659539632815056"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2efc474e-acc1-40c1-f789-97817875807f"},"execution_count":126,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Input Embeddings Shape: torch.Size([8, 128, 512])\n","✅ Layer 1 Output Shape: torch.Size([8, 128, 512])\n","✅ Output dtype: torch.float32\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","class Layer2WithMemory(nn.Module):\n","    def __init__(self, embed_dim, hidden_dim, num_heads, memory_size, dropout=0.1):\n","        super(Layer2WithMemory, self).__init__()\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads  # Ensure num_heads is set as an attribute\n","        self.memory_size = memory_size\n","\n","        # Sublayers\n","        self.pre_layer_norm = LayerNorm(embed_dim)\n","        self.self_attention = MultiHeadSelfAttention(embed_dim, num_heads)  # Pass num_heads here\n","        self.residual1 = ResidualConnection(embed_dim, dropout)\n","        self.layer_norm1 = LayerNorm(embed_dim)\n","\n","        # Memory Module\n","        self.memory_bank = nn.Parameter(torch.zeros(memory_size, embed_dim, dtype=torch.float32))  # Initialize with zeros\n","        self.memory_norm = LayerNorm(embed_dim)\n","        self.memory_attention = MultiHeadSelfAttention(embed_dim, num_heads)  # Pass num_heads here\n","\n","        # Learnable Memory Update Mechanism\n","        self.memory_gate = nn.Linear(embed_dim, 1)  # Learnable gate\n","        self.sigmoid = nn.Sigmoid()\n","\n","        # Feedforward Network\n","        self.ffn = FeedforwardNetwork(embed_dim, hidden_dim, dropout)\n","        self.residual2 = ResidualConnection(embed_dim, dropout)\n","        self.layer_norm2 = LayerNorm(embed_dim)\n","\n","    def forward(self, x):\n","        batch_size, seq_len, embed_dim = x.shape\n","        num_heads = self.memory_attention.num_heads\n","        head_dim = embed_dim // num_heads\n","\n","        # Sublayer 1: Self-Attention with Residual Connection\n","        x = self.residual1(x, self.self_attention)\n","        x = self.layer_norm1(x)\n","\n","        # Step 1: Memory Read\n","        memory_bank = self.memory_bank.unsqueeze(0).expand(batch_size, -1, -1)  # (batch, memory_size, embed_dim)\n","        memory_bank = memory_bank.type(x.dtype)\n","        memory_bank = memory_bank.view(batch_size, self.memory_size, num_heads, head_dim)  # Reshape\n","        memory_bank = memory_bank.permute(0, 2, 1, 3)  # (batch, num_heads, memory_size, head_dim)\n","\n","        Q = x.view(batch_size, seq_len, num_heads, head_dim).permute(0, 2, 1, 3)  # (batch, num_heads, seq_len, head_dim)\n","\n","        # Fixed Memory Attention\n","        memory_output = F.scaled_dot_product_attention(Q, memory_bank, memory_bank)\n","        memory_output = memory_output.permute(0, 2, 1, 3).reshape(batch_size, seq_len, embed_dim)  # Reshape back\n","\n","        # Learnable Gating for Memory Update\n","        gate_weight = self.sigmoid(self.memory_gate(x))\n","        memory_output = gate_weight * memory_output\n","\n","        x = x + memory_output  # Integrate memory output\n","\n","        # Step 2: Memory Write (Dynamic Update)\n","        self.update_memory(x)\n","\n","        # Sublayer 2: Feedforward Network with Residual Connection\n","        x = self.residual2(x, self.ffn)\n","        x = self.layer_norm2(x)\n","\n","        return x  # ✅ Fixed Output\n","\n","    def update_memory(self, x):\n","        with torch.no_grad():  # Ensure memory update doesn't affect gradients\n","            # Shift memory bank (oldest memory slot is replaced)\n","            updated_memory_bank = torch.roll(self.memory_bank, shifts=-1, dims=0)\n","\n","            # Aggregate information from current input\n","            update_value = x.mean(dim=1)  # Mean pooling over sequence length, shape: [batch_size, embed_dim]\n","\n","            # Learnable update factor\n","            gate = self.sigmoid(self.memory_gate(update_value))  # Shape: [batch_size, 1]\n","            gate = gate.expand_as(update_value)  # Shape: [batch_size, embed_dim]\n","\n","            # Expand self.memory_bank[0] to match the batch dimension\n","            memory_slot = updated_memory_bank[0].unsqueeze(0).expand_as(update_value)  # Shape: [batch_size, embed_dim]\n","\n","            # Smooth update: Blend new information with existing memory\n","            updated_memory_slot = (gate * update_value) + (1 - gate) * memory_slot\n","\n","            # Update the memory bank\n","            updated_memory_bank[0] = updated_memory_slot.mean(dim=0)  # Average over batch dimension\n","\n","            # Assign the updated memory bank\n","            self.memory_bank.data = updated_memory_bank.data  # Non-inplace update\n","\n","\n","# ✅ Hyperparameters\n","batch_size = 8\n","seq_len = 128\n","embed_dim = 512\n","hidden_dim = 2048  # FFN hidden dimension\n","num_heads = 8\n","memory_size = 100  # Memory bank size\n","\n","# ✅ Initialize Layer 2 with Memory\n","layer2_with_memory = Layer2WithMemory(embed_dim, hidden_dim, num_heads, memory_size).to(device)\n","\n","# ✅ Example Input (Layer 1 ka output)\n","input_embeddings = torch.randn(batch_size, seq_len, embed_dim, dtype=torch.float32, device=device)\n","\n","# ✅ Apply Layer 2 with Memory\n","output = layer2_with_memory(input_embeddings)\n","\n","# ✅ Debugging Info\n","print(\"✅ Input Embeddings Shape:\", input_embeddings.shape)  # Expected: (8, 128, 512)\n","print(\"✅ Layer 2 Output Shape:\", output.shape)  # Expected: (8, 128, 512)\n","print(\"✅ Output dtype:\", output.dtype)  # Expected: float32"],"metadata":{"id":"y7lyJl4MQ2LR","executionInfo":{"status":"ok","timestamp":1742710694186,"user_tz":-330,"elapsed":279,"user":{"displayName":"Ankit Kashyap","userId":"08351659539632815056"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"89b2fd06-d2c2-45a9-ac1d-bc85e8f89b76"},"execution_count":127,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Input Embeddings Shape: torch.Size([8, 128, 512])\n","✅ Layer 2 Output Shape: torch.Size([8, 128, 512])\n","✅ Output dtype: torch.float32\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# ✅ Multi-Head Self-Attention Layer\n","class MultiHeadSelfAttention(nn.Module):\n","    def __init__(self, embed_dim, num_heads):\n","        super(MultiHeadSelfAttention, self).__init__()\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads  # Store num_heads as an attribute\n","        self.head_dim = embed_dim // num_heads\n","\n","        assert self.head_dim * num_heads == embed_dim, \"Embedding dimension must be divisible by number of heads\"\n","\n","        # Fused QKV Projection (Single Linear Layer for Efficiency)\n","        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim, dtype=torch.float32)\n","        self.out_proj = nn.Linear(embed_dim, embed_dim, dtype=torch.float32)\n","\n","    def forward(self, x, mask=None):\n","        batch_size, seq_len, embed_dim = x.shape\n","\n","        # Compute Q, K, V in a single pass\n","        qkv = self.qkv_proj(x).view(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n","        Q, K, V = qkv.unbind(dim=2)  # Split into separate tensors\n","\n","        # Reshape for multi-head attention\n","        Q = Q.transpose(1, 2)  # Shape: (batch, num_heads, seq_len, head_dim)\n","        K = K.transpose(1, 2)\n","        V = V.transpose(1, 2)\n","\n","        # Flash Attention (Optimized Scaled Dot-Product Attention)\n","        if mask is not None:\n","            mask = mask.to(dtype=x.dtype, device=x.device)  # Ensure mask is on the correct device and dtype\n","        output = F.scaled_dot_product_attention(Q, K, V, attn_mask=mask)\n","\n","        # Reshape back to original shape\n","        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n","\n","        # Apply output projection\n","        return self.out_proj(output)\n","\n","# ✅ Chunked Attention (Fixed)\n","class ChunkedAttention(nn.Module):\n","    def __init__(self, embed_dim, num_heads, chunk_size=32):\n","        super(ChunkedAttention, self).__init__()\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads\n","        self.chunk_size = chunk_size\n","        self.self_attention = MultiHeadSelfAttention(embed_dim, num_heads)  # Self-Attention\n","\n","    def forward(self, x):\n","        batch_size, seq_len, embed_dim = x.shape\n","\n","        # Step 1: Pad input if necessary\n","        pad_len = (self.chunk_size - seq_len % self.chunk_size) % self.chunk_size\n","        x = F.pad(x, (0, 0, 0, pad_len))  # Pad along seq_len\n","        seq_len += pad_len\n","\n","        # Step 2: Divide input into chunks\n","        num_chunks = seq_len // self.chunk_size\n","        x = x.view(batch_size, num_chunks, self.chunk_size, embed_dim)  # (batch, num_chunks, chunk_size, embed_dim)\n","\n","        # Step 3: Apply self-attention to each chunk\n","        x = x.reshape(batch_size * num_chunks, self.chunk_size, embed_dim)  # (batch * num_chunks, chunk_size, embed_dim)\n","        x = self.self_attention(x)  # Apply self-attention\n","        x = x.reshape(batch_size, num_chunks, self.chunk_size, embed_dim)  # Reshape back\n","\n","        # Step 4: Combine chunks back into sequence\n","        x = x.reshape(batch_size, seq_len, embed_dim)  # Reshape back\n","\n","        return x[:, :seq_len - pad_len, :]  # Remove padding\n","\n","# ✅ LayerNorm Wrapper\n","class LayerNorm(nn.Module):\n","    def __init__(self, embed_dim):\n","        super(LayerNorm, self).__init__()\n","        self.norm = nn.LayerNorm(embed_dim)\n","\n","    def forward(self, x):\n","        if x.dtype != torch.float32:\n","            x = x.to(torch.float32)  # Ensure input is float32\n","        return self.norm(x)\n","\n","# ✅ Recurrent Memory (Fixed)\n","class RecurrentMemory(nn.Module):\n","    def __init__(self, embed_dim, memory_size, num_heads):  # Add num_heads as a parameter\n","        super(RecurrentMemory, self).__init__()\n","        self.memory_size = memory_size\n","        self.num_heads = num_heads  # Store num_heads as an attribute\n","        self.memory_bank = nn.Parameter(torch.zeros(memory_size, embed_dim, dtype=torch.float32))  # Use float32\n","        self.memory_norm = LayerNorm(embed_dim)  # Normalize memory output\n","        self.memory_attention = nn.MultiheadAttention(embed_dim, num_heads=num_heads, batch_first=True)  # Use num_heads\n","\n","    def forward(self, x):\n","        batch_size, seq_len, embed_dim = x.shape\n","\n","        # Step 1: Memory Read (Multi-query attention)\n","        memory_expanded = self.memory_bank.unsqueeze(0).expand(batch_size, -1, -1)  # (batch, memory_size, embed_dim)\n","        memory_output, _ = self.memory_attention(x, memory_expanded, memory_expanded)  # (batch, seq_len, embed_dim)\n","        memory_output = self.memory_norm(memory_output)  # Normalize memory output\n","\n","        # Step 2: Integrate memory output into main stream\n","        x = x + memory_output  # Add memory output to input\n","\n","        # Step 3: Memory Write (Update memory bank)\n","        self.update_memory(x)\n","\n","        return x\n","\n","    def update_memory(self, x):\n","        with torch.no_grad():\n","            # Aggregate information for update\n","            update_value = x.mean(dim=1).mean(dim=0)\n","\n","            # Create a new memory bank with shifted values\n","            updated_memory_bank = torch.roll(self.memory_bank, shifts=-1, dims=0)\n","            updated_memory_bank[-1] = update_value\n","\n","            # Update the memory bank using a non-in-place operation\n","            self.memory_bank.data = updated_memory_bank  # Assign to .data to avoid in-place operation\n","\n","\n","# ✅ Hyperparameters\n","batch_size = 8\n","seq_len = 128\n","embed_dim = 512\n","num_heads = 8\n","chunk_size = 32\n","memory_size = 10\n","\n","# ✅ Initialize Modules\n","multi_head_attention = MultiHeadSelfAttention(embed_dim, num_heads).to(device)\n","chunked_attention = ChunkedAttention(embed_dim, num_heads, chunk_size).to(device)\n","layer_norm = LayerNorm(embed_dim).to(device)\n","# ✅ Initialize Recurrent Memory\n","recurrent_memory = RecurrentMemory(embed_dim, memory_size, num_heads).to(device)\n","# ✅ Example Input (Random Token Embeddings)\n","input_embeddings = torch.randn(batch_size, seq_len, embed_dim, dtype=torch.float32, device=device)\n","\n","# ✅ Apply Multi-Head Self-Attention\n","output_multi_head_attention = multi_head_attention(input_embeddings)\n","\n","# ✅ Apply Chunked Attention\n","output_chunked_attention = chunked_attention(input_embeddings)\n","\n","# ✅ Apply LayerNorm\n","output_layer_norm = layer_norm(input_embeddings)\n","\n","# ✅ Apply Recurrent Memory\n","output_recurrent_memory = recurrent_memory(input_embeddings)\n","\n","# ✅ Debugging Info\n","print(\"✅ Input Embeddings Shape:\", input_embeddings.shape)  # Expected: (8, 128, 512)\n","print(\"✅ Multi-Head Self-Attention Output Shape:\", output_multi_head_attention.shape)  # Expected: (8, 128, 512)\n","print(\"✅ Chunked Attention Output Shape:\", output_chunked_attention.shape)  # Expected: (8, 128, 512)\n","print(\"✅ LayerNorm Output Shape:\", output_layer_norm.shape)  # Expected: (8, 128, 512)\n","print(\"✅ Recurrent Memory Output Shape:\", output_recurrent_memory.shape)  # Expected: (8, 128, 512)"],"metadata":{"id":"Xlq6MQMqQ2H1","executionInfo":{"status":"ok","timestamp":1742710694372,"user_tz":-330,"elapsed":187,"user":{"displayName":"Ankit Kashyap","userId":"08351659539632815056"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a8fdeab6-a5df-4103-ae6b-6e5d3d99cb6a"},"execution_count":128,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Input Embeddings Shape: torch.Size([8, 128, 512])\n","✅ Multi-Head Self-Attention Output Shape: torch.Size([8, 128, 512])\n","✅ Chunked Attention Output Shape: torch.Size([8, 128, 512])\n","✅ LayerNorm Output Shape: torch.Size([8, 128, 512])\n","✅ Recurrent Memory Output Shape: torch.Size([8, 128, 512])\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","# from FeedforwardNetwork import LayerNorm,MultiHeadSelfAttention,ResidualConnection,FeedforwardNetwork\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","\n","\n","\n","class RewardModel(nn.Module):\n","    def __init__(self, embed_dim):\n","        super(RewardModel, self).__init__()\n","        self.linear1 = nn.Linear(embed_dim, 256)  # First linear layer\n","        self.linear2 = nn.Linear(256, 1)  # Second linear layer\n","        self.activation = nn.ReLU()  # Activation function\n","        self.dropout = nn.Dropout(0.1)  # Dropout for regularization\n","\n","    def forward(self, x):\n","        x = self.activation(self.linear1(x))  # Apply first linear layer and activation\n","        x = self.dropout(x)  # Apply dropout\n","        x = self.linear2(x).float()  # Apply second linear layer and ensure float32 output\n","        return x  # Shape: (batch, seq_len, 1)\n","class PPOOptimizer:\n","    def __init__(self, model, reward_model, lr=1e-4, gamma=0.99, clip_epsilon=0.2, entropy_coef=0.01):\n","        self.model = model\n","        self.reward_model = reward_model\n","        self.optimizer = optim.Adam(model.parameters(), lr=lr)  # Adam optimizer\n","        self.gamma = gamma  # Discount factor\n","        self.clip_epsilon = clip_epsilon  # Clipping parameter for PPO\n","        self.entropy_coef = entropy_coef  # Entropy coefficient\n","\n","    def compute_advantages(self, rewards, values):\n","        \"\"\"\n","        Compute advantages using Generalized Advantage Estimation (GAE).\n","        \"\"\"\n","        advantages = torch.zeros_like(rewards)  # Initialize advantages\n","        last_advantage = 0  # Initialize last advantage\n","\n","        # Vectorized GAE computation\n","        for t in reversed(range(len(rewards))):\n","            next_value = values[t + 1] if t < len(rewards) - 1 else 0  # Handle last timestep\n","            delta = rewards[t] + self.gamma * next_value - values[t]  # Compute delta\n","            advantages[t] = delta + self.gamma * last_advantage  # Update advantages\n","            last_advantage = advantages[t]  # Update last advantage\n","\n","        return advantages\n","\n","    def update(self, states, actions, rewards, old_log_probs, values):\n","        print(\"🔍 PPO Update Start\")\n","\n","        advantages = self.compute_advantages(rewards, values)\n","        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n","\n","        print(f\"🔹 Advantages: {advantages.shape}\")\n","\n","        # Get outputs from the model\n","        output, new_log_probs, _, new_values, _ = self.model(states, actions)  # Unpack to match model output\n","\n","        # No need to call model again\n","        # new_log_probs, new_values, entropy = self.model(states, actions)\n","\n","        print(f\"✅ Log Probs: {new_log_probs.shape}, ✅ Values: {new_values.shape}\") # , ✅ Entropy: {entropy.shape}\n","\n","        ratio = torch.exp(new_log_probs - old_log_probs)\n","        policy_loss = -torch.min(ratio * advantages,\n","                                 torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages).mean()\n","\n","        returns = advantages + values\n","        value_loss = F.mse_loss(new_values, returns)\n","\n","        # entropy_loss = -entropy.mean() # entropy not calculated\n","        loss = policy_loss + 0.5 * value_loss # + self.entropy_coef * entropy_loss # entropy_loss removed\n","\n","        print(f\"📉 Loss: {loss.item()}\")\n","        self.loss = loss\n","\n","        self.optimizer.zero_grad()\n","        loss.backward(retain_graph = True)\n","        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.5)\n","        self.optimizer.step()\n","\n","        print(\"✅ PPO Update Done!\")\n","\n","\n","# ✅ Hyperparameters\n","batch_size = 8\n","seq_len = 128\n","embed_dim = 512\n","\n","# ✅ Initialize Reward Model\n","reward_model = RewardModel(embed_dim).to(device)\n","\n","# ✅ Example Input (Random Token Embeddings)\n","input_embeddings = torch.randn(batch_size, seq_len, embed_dim, dtype=torch.float32, device=device)\n","\n","# ✅ Apply Reward Model\n","rewards = reward_model(input_embeddings)\n","\n","# ✅ Debugging Info\n","print(\"✅ Input Embeddings Shape:\", input_embeddings.shape)  # Expected: (8, 128, 512)\n","print(\"✅ Rewards Shape:\", rewards.shape)  # Expected: (8, 128, 1)\n","print(\"✅ Rewards dtype:\", rewards.dtype)  # Expected: float32\n","\n","\n","\n"],"metadata":{"id":"9zhvMT0aQ2F0","executionInfo":{"status":"ok","timestamp":1742710694373,"user_tz":-330,"elapsed":5,"user":{"displayName":"Ankit Kashyap","userId":"08351659539632815056"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"32d22076-f756-4f85-e178-82861f590b11"},"execution_count":129,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Input Embeddings Shape: torch.Size([8, 128, 512])\n","✅ Rewards Shape: torch.Size([8, 128, 1])\n","✅ Rewards dtype: torch.float32\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# from FeedforwardNetwork import LayerNorm, MultiHeadSelfAttention, ResidualConnection, FeedforwardNetwork\n","# from chunkMemory import ChunkedAttention, RecurrentMemory\n","# from Layer3.second_of_layer3 import PPOOptimizer\n","\n","# Set device\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# Set random seed for reproducibility\n","torch.manual_seed(42)\n","\n","\n","class Layer3WithContextAndRL(nn.Module):\n","    def __init__(self, embed_dim, hidden_dim, num_heads, memory_size, dropout=0.1, num_actions=10):\n","        super(Layer3WithContextAndRL, self).__init__()\n","        # Sublayers\n","        self.pre_layer_norm = LayerNorm(embed_dim)  # Pre-LayerNorm\n","        self.self_attention = MultiHeadSelfAttention(embed_dim, num_heads)  # Self-Attention\n","        self.residual1 = ResidualConnection(embed_dim, dropout)  # Residual Connection 1\n","        self.layer_norm1 = LayerNorm(embed_dim)  # LayerNorm after Self-Attention\n","\n","        # Context Handling\n","        self.chunked_attention = ChunkedAttention(embed_dim, num_heads)  # Chunked Attention\n","        self.recurrent_memory = RecurrentMemory(embed_dim, memory_size, num_heads)  # Recurrent Memory\n","\n","        # Feedforward Network\n","        self.ffn = FeedforwardNetwork(embed_dim, hidden_dim, dropout)  # FFN\n","        self.residual2 = ResidualConnection(embed_dim, dropout)  # Residual Connection 2\n","        self.layer_norm2 = LayerNorm(embed_dim)  # LayerNorm after FFN\n","\n","        # RL Integration\n","        self.reward_model = nn.Linear(embed_dim, 1)  # Reward Model\n","        self.policy_network = nn.Linear(embed_dim, num_actions)  # Policy Network\n","        self.value_network = nn.Linear(embed_dim, 1)  # Value Network\n","\n","        # PPO Optimizer\n","        self.ppo_optimizer = PPOOptimizer(model=self, reward_model=self.reward_model)  # Initialize PPO Optimizer\n","\n","    def forward(self, x, actions=None):\n","        # Sublayer 1: Self-Attention with Residual Connection\n","        x = self.residual1(x, self.self_attention)\n","        x = self.layer_norm1(x)\n","\n","        # Context Handling\n","        x = self.chunked_attention(x)\n","        x = self.recurrent_memory(x)\n","\n","        # Sublayer 2: Feedforward Network with Residual Connection\n","        x = self.residual2(x, self.ffn)\n","        x = self.layer_norm2(x)\n","\n","        # RL Integration: Compute Rewards\n","        rewards = self.reward_model(x)  # Shape: (batch_size, seq_len, 1)\n","\n","        # Policy Network (Action Selection)\n","        last_hidden_state = x[:, -1, :]  # Use the last hidden state for action selection\n","        logits = self.policy_network(last_hidden_state)\n","        probs = F.softmax(logits, dim=-1)\n","\n","        # If actions are not provided, sample from policy\n","        if actions is None:\n","            actions = torch.multinomial(probs, num_samples=1).squeeze(-1)\n","\n","        # Compute log probs for selected actions\n","        new_log_probs = torch.log(probs.gather(-1, actions.unsqueeze(-1))).squeeze(-1)\n","\n","        # Value Network (State Value Estimation)\n","        values = self.value_network(last_hidden_state).squeeze(-1)\n","\n","        return x, new_log_probs, actions, values, rewards  # Return all five values\n","\n","\n","\n","\n","\n","# ✅ Hyperparameters\n","batch_size = 8\n","seq_len = 128\n","embed_dim = 512\n","hidden_dim = 2048  # FFN hidden dimension\n","num_heads = 8\n","memory_size = 100  # Memory bank size\n","num_actions = 10  # Number of actions\n","\n","# ✅ Initialize Layer 3 with Context and RL\n","layer3_with_context_and_rl = Layer3WithContextAndRL(\n","    embed_dim=embed_dim,\n","    hidden_dim=hidden_dim,\n","    num_heads=num_heads,\n","    memory_size=memory_size,\n","    num_actions=num_actions\n",").to(device)\n","\n","# ✅ Example Input (Layer 2 ka output)\n","input_embeddings = torch.randn(batch_size, seq_len, embed_dim, dtype=torch.float32, device=device)\n","\n","# ✅ Apply Layer 3 with Context and RL\n","output, new_log_probs, actions, values, rewards = layer3_with_context_and_rl(input_embeddings)\n","\n","# ✅ Debugging Info\n","print(\"✅ Input Embeddings Shape:\", input_embeddings.shape)  # Expected: (8, 128, 512)\n","print(\"✅ Layer 3 Output Shape:\", output.shape)  # Expected: (8, 128, 512)\n","print(\"✅ New Log Probs Shape:\", new_log_probs.shape)  # Expected: (8,)\n","print(\"✅ Actions Shape:\", actions.shape)  # Expected: (8,)\n","print(\"✅ Values Shape:\", values.shape)  # Expected: (8,)\n","print(\"✅ Rewards Shape:\", rewards.shape)  # Expected: (8, 128, 1)\n","print(\"✅ Output dtype:\", output.dtype)  # Expected: float32"],"metadata":{"id":"u9nXUqJSQ2Ca","executionInfo":{"status":"ok","timestamp":1742710694790,"user_tz":-330,"elapsed":419,"user":{"displayName":"Ankit Kashyap","userId":"08351659539632815056"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"287b45c7-59cf-4c0b-f9f2-f085c247d95d"},"execution_count":130,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Input Embeddings Shape: torch.Size([8, 128, 512])\n","✅ Layer 3 Output Shape: torch.Size([8, 128, 512])\n","✅ New Log Probs Shape: torch.Size([8])\n","✅ Actions Shape: torch.Size([8])\n","✅ Values Shape: torch.Size([8])\n","✅ Rewards Shape: torch.Size([8, 128, 1])\n","✅ Output dtype: torch.float32\n"]}]},{"cell_type":"code","source":["import os\n","import torch\n","from torch.amp import GradScaler, autocast\n","from torch.utils.data import Dataset, DataLoader\n","import json\n","from pathlib import Path\n","\n","# Enable synchronous CUDA execution for better debugging\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n","\n","# Set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using {device} for training 🔥\")\n","\n","# Import custom modules\n","# from TokenizationLayer import TokenizationLayer\n","# from TokenEmbedding import TokenEmbedding\n","# from Layer1Stack import Layer1\n","# from Layer2WithMemory import Layer2WithMemory\n","# from Layer3.third_of_layer3_stacking import Layer3WithContextAndRL\n","\n","\n","# ----------------------------\n","# 1. Dataset Class - Handles both text and JSON files\n","# ----------------------------\n","class BhaiDataset(Dataset):\n","    def __init__(self, file_path):\n","        self.file_path = Path(file_path)\n","        self.data = self._load_data()\n","\n","    def _load_data(self):\n","        # Load data based on file type\n","        if self.file_path.suffix == '.txt':\n","            return self._load_txt()\n","        elif self.file_path.suffix == '.json':\n","            return self._load_json()\n","        else:\n","            raise ValueError(\"Bhai, only .txt or .json files are supported!\")\n","\n","    def _load_txt(self):\n","        with open(self.file_path, 'r', encoding='utf-8', errors='replace') as f:\n","            return [self._clean(line) for line in f if line.strip()]\n","\n","    def _load_json(self):\n","        with open(self.file_path, 'r', encoding='utf-8') as f:\n","            data = json.load(f)\n","            return self._parse_json(data)\n","\n","    def _clean(self, text):\n","        # Clean text by removing unwanted characters\n","        bad_chars = {'\\x00', '\\ufffd', '�', '\\r'}\n","        return ''.join(c for c in text if c not in bad_chars).strip()\n","\n","    def _parse_json(self, data):\n","        # Parse JSON data (supports both simple and nested JSON)\n","        texts = []\n","        if isinstance(data, list):\n","            for item in data:\n","                if 'text' in item:\n","                    texts.append(self._clean(item['text']))\n","                elif 'content' in item:\n","                    texts.append(self._clean(item['content']))\n","        elif isinstance(data, dict):\n","            for key, value in data.items():\n","                if isinstance(value, str):\n","                    texts.append(self._clean(value))\n","        return texts\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        return self.data[idx]\n","\n","\n","# ----------------------------\n","# 2. Training Loop (GPU optimized)\n","# ----------------------------\n","def bhai_trainer(dataset_path, epochs=10):\n","    # Initialize dataset and tokenizer\n","    dataset = BhaiDataset(dataset_path)\n","    tokenizer = TokenizationLayer()\n","\n","    # DataLoader with smart collate function\n","    def collate_fn(batch):\n","        tokens = tokenizer.tokenize(batch, max_length=128)  # Reduced sequence length\n","        return tokens.to(device)\n","\n","    dataloader = DataLoader(dataset, batch_size=2, collate_fn=collate_fn, shuffle=True)  # Reduced batch size\n","\n","    # Initialize model components\n","    embedder = TokenEmbedding(vocab_size=tokenizer.vocab_size, embed_dim=256).to(device)\n","    layer1 = Layer1(256, 1024, 8).to(device)\n","    layer2 = Layer2WithMemory(256, 1024, 8, 100).to(device)\n","    layer3 = Layer3WithContextAndRL(256, 1024, 8, 100).to(device)\n","\n","    # Optimizer and AMP setup\n","    optimizer = layer3.ppo_optimizer.optimizer\n","    scaler = GradScaler()  # Correct initialization\n","\n","    # Training loop\n","    for epoch in range(epochs):\n","        for batch_idx, inputs in enumerate(dataloader):\n","            with autocast(device_type = 'cuda'):  # Mixed precision\n","                # Forward pass\n","                emb = embedder(inputs)\n","                print(f\"Embeddings shape: {emb.shape}\")  # Debugging\n","                l1 = layer1(emb)\n","                print(f\"Layer 1 output shape: {l1.shape}\")  # Debugging\n","                l2 = layer2(l1)\n","                print(f\"Layer 2 output shape: {l2.shape}\")  # Debugging\n","                l3, log_probs, actions, values, rewards = layer3(l2)  # Unpack all five values\n","                print(f\"Layer 3 output shape: {l3.shape}\")  # Debugging\n","\n","                # PPO update\n","                layer3.ppo_optimizer.update(\n","                    states=l2,\n","                    actions=actions,\n","                    rewards=rewards.mean(dim=1),  # Use rewards from layer3\n","                    old_log_probs=log_probs.detach(),\n","                    values=values\n","                )\n","\n","            loss = layer3.ppo_optimizer.loss\n","\n","            # Backpropagation\n","            scaler.scale(loss).backward()\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","\n","            # Print progress\n","            if batch_idx % 10 == 0:\n","                gpu_mem = torch.cuda.memory_allocated() // 1024 ** 2\n","                print(f\"Epoch {epoch + 1} | Batch {batch_idx} | Loss: {loss.item():.2f} | GPU Mem: {gpu_mem}MB\")\n","\n","        # Save model checkpoint\n","        torch.save({\n","            'layer1': layer1.state_dict(),\n","            'layer2': layer2.state_dict(),\n","            'layer3': layer3.state_dict(),\n","            'embedder': embedder.state_dict()\n","        }, f\"bhai_llm_epoch_{epoch + 1}.pt\")\n","\n","\n","# ----------------------------\n","# 3. Main Execution\n","# ----------------------------\n","if __name__ == \"__main__\":\n","    import argparse\n","\n","    # Set up argument parser\n","    parser = argparse.ArgumentParser(description='Bhai ka LLM Trainer')\n","    parser.add_argument('--dataset', type=str, required=True, help='Path to .txt or .json file')\n","    parser.add_argument('--epochs', type=int, default=10, help='Number of epochs')\n","\n","    # For testing, hardcode the dataset path\n","    args = parser.parse_args([\n","        '--dataset', '/content/100.txt',  # Replace with your dataset path\n","        '--epochs', '10'\n","    ])\n","\n","    # Start training\n","    print(f\"\\nBhai, training shuru kar raha hoon 🚀\")\n","    print(f\"Dataset: {args.dataset}\")\n","    print(f\"Epochs: {args.epochs}\\n\")\n","\n","    bhai_trainer(args.dataset, args.epochs)"],"metadata":{"id":"3UMoiuvdQ1_E","executionInfo":{"status":"error","timestamp":1742710695677,"user_tz":-330,"elapsed":869,"user":{"displayName":"Ankit Kashyap","userId":"08351659539632815056"}},"colab":{"base_uri":"https://localhost:8080/","height":825},"outputId":"23071dff-a5c2-424b-c178-eab4e7e27974"},"execution_count":131,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cpu for training 🔥\n","\n","Bhai, training shuru kar raha hoon 🚀\n","Dataset: /content/100.txt\n","Epochs: 10\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n","  warnings.warn(\n","<ipython-input-129-5997fbdc3f7d>:72: UserWarning: Using a target size (torch.Size([2, 2])) that is different to the input size (torch.Size([2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  value_loss = F.mse_loss(new_values, returns)\n"]},{"output_type":"stream","name":"stdout","text":["Embeddings shape: torch.Size([2, 128, 256])\n","Layer 1 output shape: torch.Size([2, 128, 256])\n","Layer 2 output shape: torch.Size([2, 128, 256])\n","Layer 3 output shape: torch.Size([2, 128, 256])\n","🔍 PPO Update Start\n","🔹 Advantages: torch.Size([2, 1])\n","✅ Log Probs: torch.Size([2]), ✅ Values: torch.Size([2])\n","📉 Loss: 0.5688914060592651\n","✅ PPO Update Done!\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [256, 1]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-131-a2e6d029fe4e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epochs: {args.epochs}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0mbhai_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-131-a2e6d029fe4e>\u001b[0m in \u001b[0;36mbhai_trainer\u001b[0;34m(dataset_path, epochs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [256, 1]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."]}]},{"cell_type":"code","source":[],"metadata":{"id":"JAqpFxbBQ19J","executionInfo":{"status":"aborted","timestamp":1742710695675,"user_tz":-330,"elapsed":9643,"user":{"displayName":"Ankit Kashyap","userId":"08351659539632815056"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7XF3kvPfQ17l","executionInfo":{"status":"aborted","timestamp":1742710695680,"user_tz":-330,"elapsed":9645,"user":{"displayName":"Ankit Kashyap","userId":"08351659539632815056"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nHL0g7bPQ14w","executionInfo":{"status":"aborted","timestamp":1742710695712,"user_tz":-330,"elapsed":13,"user":{"displayName":"Ankit Kashyap","userId":"08351659539632815056"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QDvVM4j8Q11a","executionInfo":{"status":"aborted","timestamp":1742710695740,"user_tz":-330,"elapsed":25,"user":{"displayName":"Ankit Kashyap","userId":"08351659539632815056"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Bz_s-PeAQ1yt","executionInfo":{"status":"aborted","timestamp":1742710695744,"user_tz":-330,"elapsed":9703,"user":{"displayName":"Ankit Kashyap","userId":"08351659539632815056"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yOfdoZ4wQ1u7","executionInfo":{"status":"aborted","timestamp":1742710695745,"user_tz":-330,"elapsed":9702,"user":{"displayName":"Ankit Kashyap","userId":"08351659539632815056"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"YqpI6LMUQ1rU","executionInfo":{"status":"aborted","timestamp":1742710695747,"user_tz":-330,"elapsed":9702,"user":{"displayName":"Ankit Kashyap","userId":"08351659539632815056"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"uy_lxNccQ1l5","executionInfo":{"status":"aborted","timestamp":1742710695748,"user_tz":-330,"elapsed":9700,"user":{"displayName":"Ankit Kashyap","userId":"08351659539632815056"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","\n","def generate_text(model, tokenizer, prompt, max_length=50, temperature=1.0, top_k=50, top_p=0.95):\n","    \"\"\"\n","    Generate text using a trained model with temperature scaling, top-k filtering, and top-p sampling.\n","\n","    Args:\n","        model: The trained model.\n","        tokenizer: The tokenizer for encoding/decoding text.\n","        prompt: The input prompt (string).\n","        max_length: Maximum length of the generated sequence.\n","        temperature: Temperature for scaling logits (higher = more random).\n","        top_k: Top-k filtering (0 to disable).\n","        top_p: Top-p (nucleus) sampling (0.0 to disable).\n","\n","    Returns:\n","        generated_text: The generated text (string).\n","    \"\"\"\n","    model.eval()  # Set model to evaluation mode\n","    device = next(model.parameters()).device  # Get the device of the model\n","\n","    # Tokenize input prompt\n","    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n","    generated_ids = input_ids.clone()  # Store generated tokens\n","\n","    with torch.no_grad():  # Disable gradient calculation\n","        for _ in range(max_length):\n","            # Forward Pass\n","            outputs = model(input_ids)\n","            logits = outputs.logits[:, -1, :]  # Get logits for the last token\n","\n","            # Apply temperature scaling\n","            if temperature != 1.0:\n","                logits = logits / temperature\n","\n","            # Top-k Filtering\n","            if top_k > 0:\n","                top_k_values, _ = torch.topk(logits, top_k)\n","                logits[logits < top_k_values[:, -1].unsqueeze(-1)] = -float('inf')\n","\n","            # Top-p (Nucleus) Sampling\n","            if top_p > 0.0:\n","                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n","                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n","                sorted_indices_to_remove = cumulative_probs > top_p\n","                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n","                sorted_indices_to_remove[..., 0] = 0\n","                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n","                logits[:, indices_to_remove] = -float('inf')\n","\n","            # Sample next token\n","            probs = F.softmax(logits, dim=-1)\n","            next_token = torch.multinomial(probs, num_samples=1)\n","\n","            # Append generated token to input\n","            input_ids = torch.cat([input_ids, next_token], dim=-1)\n","            generated_ids = torch.cat([generated_ids, next_token], dim=-1)\n","\n","            # Stop if end-of-sequence token is generated\n","            if next_token.item() == tokenizer.eos_token_id:\n","                break\n","\n","    # Decode generated tokens to text\n","    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n","    return generated_text"],"metadata":{"id":"V-p3ZHFTM5BF","executionInfo":{"status":"aborted","timestamp":1742710695749,"user_tz":-330,"elapsed":9699,"user":{"displayName":"Ankit Kashyap","userId":"08351659539632815056"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","\n","# Load pre-trained model and tokenizer\n","model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","\n","# Generate text\n","prompt = \"Once upon a time\"\n","generated_text = generate_text(model, tokenizer, prompt, max_length=50, temperature=0.7, top_k=50, top_p=0.9)\n","print(generated_text)"],"metadata":{"id":"ZhG0KDy_M-kB","executionInfo":{"status":"aborted","timestamp":1742710695750,"user_tz":-330,"elapsed":9697,"user":{"displayName":"Ankit Kashyap","userId":"08351659539632815056"}}},"execution_count":null,"outputs":[]}]}