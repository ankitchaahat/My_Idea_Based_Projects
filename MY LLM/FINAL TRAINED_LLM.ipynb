{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyP+OsUcnlK+tPZ1oz5BZ2zv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TPdr6Z4JS3eA","executionInfo":{"status":"ok","timestamp":1749754155556,"user_tz":-330,"elapsed":22,"user":{"displayName":"Ankit kashyap","userId":"13344809073766364463"}},"outputId":"247b24a9-7dde-41df-a837-fb1310e557b4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using Device: cuda\n","cuDNN Enabled: True\n","\n","Single Example:\n","Text: Hello, my name is Ankit Kashyap\n","Tokens: tensor([[   27,  9906,    11,   856,   836,   374,  1556,  8390, 42708,    88]],\n","       device='cuda:0')\n","Decoded: ['<Hello, my name is Ankit Kashy']\n","\n","Batch Example:\n","Texts: ['I have done my homework', 'This is the perfect code!']\n","Tokens: tensor([[    27,     40,    617,   2884,    856,  29559, 100257, 100257, 100257,\n","         100257],\n","        [    27,   2028,    374,    279,   4832,   2082,      0, 100257, 100257,\n","         100257]], device='cuda:0')\n","Decoded: ['<I have done my homework', '<This is the perfect code!']\n"]}],"source":["import tiktoken\n","import torch\n","\n","\n","class TokenizationLayer:\n","    def __init__(self, model_name=\"cl100k_base\", device=None):\n","        \"\"\"\n","        Tokenization Layer using tiktoken.\n","\n","        Args:\n","            model_name (str): Tokenizer model name.\n","            device (str): 'cuda' or 'cpu'. Auto-detects if None.\n","        \"\"\"\n","        self.device = torch.device(device or (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n","\n","        # Load tokenizer\n","        self.tokenizer = tiktoken.get_encoding(model_name)\n","        self.vocab_size = self.tokenizer.n_vocab\n","\n","        # Use supported special tokens\n","        self.cls_token = \"<|startoftext|>\"\n","        self.sep_token = \"<|endoftext|>\"\n","\n","        # Explicitly allow special tokens during encoding to get their IDs\n","        self.cls_token_id = self.tokenizer.encode(self.cls_token, allowed_special={self.cls_token})[0]\n","        self.sep_token_id = self.tokenizer.encode(self.sep_token, allowed_special={self.sep_token})[0]\n","        self.pad_token_id = self.sep_token_id  # Use SEP as pad\n","        self.eos_token_id = self.sep_token_id # Use SEP as EOS token for generation\n","\n","    def tokenize(self, texts, max_length=512, add_special_tokens=True):\n","        \"\"\"\n","        Tokenizes input text(s) into padded/truncated token IDs.\n","\n","        Args:\n","            texts (str or List[str]): Text(s) to tokenize.\n","            max_length (int): Max token length.\n","            add_special_tokens (bool): Whether to add CLS + SEP.\n","\n","        Returns:\n","            torch.Tensor: Token IDs [batch_size, max_length]\n","        \"\"\"\n","        if isinstance(texts, str):\n","            texts = [texts]\n","\n","        token_ids = []\n","        for text in texts:\n","            # Allow all special tokens during the main encoding process as they might appear in the data\n","            tokens = self.tokenizer.encode(text, allowed_special=\"all\")\n","\n","            if add_special_tokens:\n","                tokens = [self.cls_token_id] + tokens + [self.sep_token_id]\n","\n","            tokens = tokens[:max_length]\n","            if len(tokens) < max_length:\n","                tokens += [self.pad_token_id] * (max_length - len(tokens))\n","\n","            token_ids.append(tokens)\n","\n","        return torch.tensor(token_ids, dtype=torch.long, device=self.device)\n","\n","    def detokenize(self, tokens):\n","        \"\"\"\n","        Converts token IDs back to string(s), removing padding.\n","\n","        Args:\n","            tokens (List[int] or Tensor): Token IDs\n","\n","        Returns:\n","            str or List[str]: Decoded text(s)\n","        \"\"\"\n","        if isinstance(tokens, torch.Tensor):\n","            tokens = tokens.cpu().tolist()\n","\n","        if isinstance(tokens[0], list):  # Batch\n","            # Remove the 'allowed_special' argument from decode\n","            return [\n","                self.tokenizer.decode([t for t in seq if t != self.pad_token_id])\n","                for seq in tokens\n","            ]\n","        else:\n","            # Remove the 'allowed_special' argument from decode\n","            return self.tokenizer.decode([t for t in tokens if t != self.pad_token_id])\n","\n","\n","# âœ… Device Info\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using Device:\", device)\n","if torch.backends.cudnn.is_available():\n","    print(\"cuDNN Enabled:\", torch.backends.cudnn.enabled)\n","\n","# âœ… Test\n","tokenizer_layer = TokenizationLayer(device=device)\n","\n","# Single\n","text = \"Hello, my name is Ankit Kashyap\"\n","tokens = tokenizer_layer.tokenize(text, max_length=10)\n","decoded = tokenizer_layer.detokenize(tokens)\n","\n","# Batch\n","texts = [\"I have done my homework\", \"This is the perfect code!\"]\n","batch_tokens = tokenizer_layer.tokenize(texts, max_length=10)\n","decoded_batch = tokenizer_layer.detokenize(batch_tokens)\n","\n","# âœ… Output\n","print(\"\\nSingle Example:\")\n","print(\"Text:\", text)\n","print(\"Tokens:\", tokens)\n","print(\"Decoded:\", decoded)\n","\n","print(\"\\nBatch Example:\")\n","print(\"Texts:\", texts)\n","print(\"Tokens:\", batch_tokens)\n","print(\"Decoded:\", decoded_batch)"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","# from Tokenization.tokenization import TokenizationLayer\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","class TokenEmbedding(nn.Module):\n","    def __init__(self, vocab_size, embed_dim, padding_idx=0, dropout=0.1):\n","        \"\"\"\n","        Token Embedding Layer using PyTorch nn.Embedding.\n","\n","        Args:\n","            vocab_size (int): Number of unique tokens in vocabulary.\n","            embed_dim (int): Dimension of each token embedding.\n","            padding_idx (int, optional): Index of padding token. Default: 0.\n","            dropout (float, optional): Dropout probability. Default: 0.1.\n","        \"\"\"\n","        super(TokenEmbedding, self).__init__()\n","        self.embedding = nn.Embedding(\n","            num_embeddings=vocab_size,\n","            embedding_dim=embed_dim,\n","            padding_idx=padding_idx  # Helps handle padding tokens efficiently\n","        )\n","        self.dropout = nn.Dropout(dropout)  # Dropout for regularization\n","        nn.init.xavier_uniform_(self.embedding.weight)  # Better initialization\n","\n","    def forward(self, input_tokens):\n","        \"\"\"\n","        Forward pass to convert token IDs to embeddings.\n","\n","        Args:\n","            input_tokens (torch.Tensor): Tensor of shape (batch_size, seq_len).\n","\n","        Returns:\n","            torch.Tensor: Token embeddings of shape (batch_size, seq_len, embed_dim).\n","        \"\"\"\n","        # Ensure input is on the correct device\n","        input_tokens = input_tokens.to(self.embedding.weight.device)\n","\n","        # Apply embedding and dropout\n","\n","        embeddings = self.embedding(input_tokens)\n","        embeddings = self.dropout(embeddings)  # Apply dropout\n","\n","        return embeddings\n","\n","\n","# âœ… Hyperparameters\n","vocab_size = 10000  # Size of vocabulary\n","embed_dim = 512  # Embedding dimension per token\n","batch_size = 8  # Number of sequences processed in parallel\n","seq_len = 128  # Max sequence length\n","\n","\n","\n","tokenizer_layer = TokenizationLayer()\n","# âœ… Initialize Token Embedding Layer\n","# âœ… Initialize Token Embedding Layer with correct vocab size\n","token_embedding = TokenEmbedding(\n","    vocab_size=tokenizer_layer.vocab_size,  # Use actual vocab size\n","    embed_dim=512\n",").to(device)\n","\n","# âœ… Example Input (Random Token IDs)\n","\n","input_tokens = torch.randint(0, tokenizer_layer.vocab_size, (batch_size, seq_len), device=device, dtype=torch.long)\n","\n","\n","# âœ… Apply Token Embedding\n","output_embeddings = token_embedding(input_tokens)\n","\n","# âœ… Debugging Info\n","print(\"\\nâœ… Input Tokens Shape:\", input_tokens.shape)  # Expected: (8, 128)\n","print(\"âœ… Token Embedding Output Shape:\", output_embeddings.shape)  # Expected: (8, 128, 512)\n","print(\"âœ… Token Embedding Output dtype:\", output_embeddings.dtype)  # Should be float16 if AMP enabled\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6ta2r4piS960","executionInfo":{"status":"ok","timestamp":1749754156911,"user_tz":-330,"elapsed":1353,"user":{"displayName":"Ankit kashyap","userId":"13344809073766364463"}},"outputId":"5a36bd46-c1a9-4dd5-dccd-ba2ea5d2ca9f"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","âœ… Input Tokens Shape: torch.Size([8, 128])\n","âœ… Token Embedding Output Shape: torch.Size([8, 128, 512])\n","âœ… Token Embedding Output dtype: torch.float32\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","\n","\n","\n","class RotaryPositionalEncoding(nn.Module):\n","    def __init__(self, embed_dim):\n","        \"\"\"\n","        Rotary Positional Encoding (RoPE) for transformers.\n","\n","        Args:\n","            embed_dim (int): Dimension of token embeddings.\n","        \"\"\"\n","        super(RotaryPositionalEncoding, self).__init__()\n","        self.embed_dim = embed_dim\n","\n","        # Compute inverse frequency terms for RoPE\n","        inv_freq = 1.0 / (10000 ** (torch.arange(0, embed_dim, 2, dtype=torch.float32) / embed_dim))\n","        self.register_buffer(\"inv_freq\", inv_freq)  # Store as buffer\n","\n","    def rotate_half(self, x):\n","        \"\"\"\n","        Rotates the last dimension by 90 degrees.\n","\n","        Args:\n","            x (torch.Tensor): Input tensor of shape (..., embed_dim).\n","\n","        Returns:\n","            torch.Tensor: Rotated tensor of same shape.\n","        \"\"\"\n","        x1, x2 = x.chunk(2, dim=-1)\n","        return torch.cat((-x2, x1), dim=-1)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass for RoPE.\n","\n","        Args:\n","            x (torch.Tensor): Token embeddings of shape (batch_size, seq_len, embed_dim).\n","\n","        Returns:\n","            torch.Tensor: Rotated embeddings with positional information.\n","        \"\"\"\n","        batch_size, seq_len, embed_dim = x.shape\n","\n","        # Generate position indices\n","        positions = torch.arange(seq_len, dtype=torch.float32, device=x.device).unsqueeze(1)\n","\n","        # Compute rotation frequencies\n","        freqs = torch.matmul(positions, self.inv_freq.unsqueeze(0))  # Shape: [seq_len, embed_dim//2]\n","        emb = torch.cat((freqs, freqs), dim=-1)  # Shape: [seq_len, embed_dim]\n","\n","        # Compute cos and sin embeddings\n","        cos_emb, sin_emb = emb.cos().unsqueeze(0), emb.sin().unsqueeze(0)  # Shape: [1, seq_len, embed_dim]\n","\n","        # Apply RoPE transformation\n","        x_rotated = (x * cos_emb) + (self.rotate_half(x) * sin_emb)\n","\n","        return x_rotated\n","\n","\n","# âœ… Hyperparameters\n","batch_size = 8\n","seq_len = 128\n","embed_dim = 512\n","\n","# âœ… Initialize RoPE\n","rotary_pe = RotaryPositionalEncoding(embed_dim).to(device)\n","\n","# âœ… Example Input (Random Token Embeddings)\n","input_embeddings = torch.randn(batch_size, seq_len, embed_dim, dtype=torch.float32, device=device)\n","\n","# âœ… Apply RoPE\n","output_embeddings = rotary_pe(input_embeddings)\n","\n","# âœ… Debugging Info\n","print(\"âœ… Input Embeddings Shape:\", input_embeddings.shape)  # Expected: (8, 128, 512)\n","print(\"âœ… RoPE Output Shape:\", output_embeddings.shape)  # Expected: (8, 128, 512)\n","print(\"âœ… RoPE Output dtype:\", output_embeddings.dtype)  # Expected: float32"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qBa1OcttS93R","executionInfo":{"status":"ok","timestamp":1749754156912,"user_tz":-330,"elapsed":22,"user":{"displayName":"Ankit kashyap","userId":"13344809073766364463"}},"outputId":"97a61c53-110b-4b03-de4c-d5be64461c39"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Input Embeddings Shape: torch.Size([8, 128, 512])\n","âœ… RoPE Output Shape: torch.Size([8, 128, 512])\n","âœ… RoPE Output dtype: torch.float32\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","# from RotaryPositionalEncoding.RotaryPositionalEncoding import  RotaryPositionalEncoding\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","class LayerNorm(nn.Module):\n","    def __init__(self, embed_dim, eps=1e-5):\n","        super(LayerNorm, self).__init__()\n","        self.gamma = nn.Parameter(torch.ones(embed_dim, dtype=torch.float32))\n","        self.beta = nn.Parameter(torch.zeros(embed_dim, dtype=torch.float32))\n","        self.eps = eps\n","\n","    def forward(self, x):\n","        if x.dtype != torch.float32:\n","            x = x.to(torch.float32)\n","        return F.layer_norm(x, x.shape[-1:], self.gamma, self.beta, self.eps)\n","\n","\n","# âœ… Residual Connection with Pre-LN\n","class ResidualConnection(nn.Module):\n","    def __init__(self, embed_dim, dropout=0.1):\n","        super(ResidualConnection, self).__init__()\n","        self.norm = LayerNorm(embed_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, sublayer):\n","        return x + self.dropout(sublayer(self.norm(x)))\n","\n","\n","# âœ… Multi-Head Self-Attention with RoPE\n","class MultiHeadSelfAttention(nn.Module):\n","    def __init__(self, embed_dim, num_heads): # Removed rotary_pe=None from init\n","        super(MultiHeadSelfAttention, self).__init__()\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads\n","        self.head_dim = embed_dim // num_heads\n","        assert self.head_dim * num_heads == embed_dim, \"Embed dim must be divisible by heads\"\n","\n","        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim, dtype=torch.float32)\n","        self.out_proj = nn.Linear(embed_dim, embed_dim, dtype=torch.float32)\n","\n","        # Initialize RoPE here using head_dim\n","        self.rotary_pe = RotaryPositionalEncoding(self.head_dim)\n","\n","    def apply_rope(self, x):\n","        # x has shape (B, H, T, head_dim)\n","        B, H, T, D = x.shape # D is head_dim\n","        # Reshape to (B * H, T, head_dim)\n","        x = x.reshape(B * H, T, D)\n","        # Apply RoPE, which expects input dimension to match its initialized dimension (head_dim)\n","        x = self.rotary_pe(x)\n","        # Reshape back to (B, H, T, head_dim)\n","        return x.reshape(B, H, T, D)\n","\n","    def forward(self, x, mask=None):\n","        B, T, _ = x.shape # x has shape (B, T, embed_dim)\n","        qkv = self.qkv_proj(x).view(B, T, 3, self.num_heads, self.head_dim)\n","        Q, K, V = qkv.unbind(dim=2) # Q, K, V are (B, T, num_heads, head_dim)\n","        Q = Q.transpose(1, 2) # (B, num_heads, T, head_dim)\n","        K = K.transpose(1, 2) # (B, num_heads, T, head_dim)\n","        V = V.transpose(1, 2) # (B, num_heads, T, head_dim)\n","\n","        # Apply RoPE to Q and K (which now have head_dim as the last dimension)\n","        Q = self.apply_rope(Q)\n","        K = self.apply_rope(K)\n","\n","        if mask is not None:\n","            # Expand mask to match (B, num_heads, T, T) for broadcasting\n","            mask = mask.to(dtype=torch.bool, device=x.device)  # bool mask is expected by scaled_dot_product_attention\n","\n","\n","        # F.scaled_dot_product_attention expects Q, K, V of shape (B, num_heads, T, head_dim)\n","        output = F.scaled_dot_product_attention(Q, K, V, attn_mask=mask) # output is (B, num_heads, T, head_dim)\n","\n","        # Reshape output back to (B, T, embed_dim)\n","        output = output.transpose(1, 2).contiguous().reshape(B, T, self.embed_dim)\n","        return self.out_proj(output)\n","\n","\n","# âœ… Hyperparameters\n","batch_size = 8\n","seq_len = 128\n","embed_dim = 512\n","num_heads = 8\n","\n","# Initialize Layers\n","# Now, RotaryPositionalEncoding is initialized inside MultiHeadSelfAttention with head_dim\n","self_attention = MultiHeadSelfAttention(embed_dim=embed_dim, num_heads=num_heads).to(device)\n","residual = ResidualConnection(embed_dim).to(device)\n","\n","# âœ… Example Input\n","input_embeddings = torch.randn(batch_size, seq_len, embed_dim, dtype=torch.float32, device=device)\n","# Create the casual mask with shape (1, 1, seq_len, seq_len)\n","mask = torch.tril(torch.ones(seq_len, seq_len, dtype=torch.bool, device=device)).unsqueeze(0).unsqueeze(0)\n","\n","# âœ… Forward Pass\n","output = residual(input_embeddings, lambda x: self_attention(x, mask=mask))\n","\n","# âœ… Debug\n","print(\"âœ… Input:\", input_embeddings.shape)      # (8, 128, 512)\n","print(\"âœ… Output:\", output.shape)               # (8, 128, 512)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LfVcBSmcS91m","executionInfo":{"status":"ok","timestamp":1749754156918,"user_tz":-330,"elapsed":22,"user":{"displayName":"Ankit kashyap","userId":"13344809073766364463"}},"outputId":"694dde83-fdca-41f3-f94c-a7500364e407"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Input: torch.Size([8, 128, 512])\n","âœ… Output: torch.Size([8, 128, 512])\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# from MultiHeadSelfAttention.MultiHeadSelfAttention import MultiHeadSelfAttention\n","\n","\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","torch.manual_seed(42)\n","\n","# âœ… Optimized Layer Normalization (Pre-LN)\n","class LayerNorm(nn.Module):\n","    def __init__(self, embed_dim, eps=1e-5):\n","        super(LayerNorm, self).__init__()\n","        self.gamma = nn.Parameter(torch.ones(embed_dim, dtype=torch.float32))  # Learnable scale\n","        self.beta = nn.Parameter(torch.zeros(embed_dim, dtype=torch.float32))  # Learnable shift\n","        self.eps = eps  # Small value for numerical stability\n","\n","    def forward(self, x):\n","        # Apply Layer Normalization\n","        return F.layer_norm(x, x.shape[-1:], self.gamma, self.beta, self.eps)\n","\n","# âœ… Optimized Feedforward Network (FFN)\n","class FeedforwardNetwork(nn.Module):\n","    def __init__(self, embed_dim, hidden_dim, dropout=0.1):\n","        \"\"\"\n","        Feedforward Network with GELU Activation and Dropout.\n","        \"\"\"\n","        super(FeedforwardNetwork, self).__init__()\n","        self.linear1 = nn.Linear(embed_dim, hidden_dim, dtype=torch.float32)  # Expansion\n","        self.linear2 = nn.Linear(hidden_dim, embed_dim, dtype=torch.float32)  # Compression\n","        self.dropout = nn.Dropout(dropout)  # Regularization\n","        self.activation = nn.GELU()  # Activation function\n","\n","    def forward(self, x):\n","        x = self.linear1(x)  # Expand dimensions\n","        x = self.activation(x)  # Apply GELU\n","        x = self.dropout(x)  # Apply dropout\n","        x = self.linear2(x)  # Compress dimensions\n","        return x\n","\n","# âœ… Optimized Residual Connection with Pre-Norm\n","class ResidualConnection(nn.Module):\n","    def __init__(self, embed_dim, dropout=0.1):\n","        \"\"\"\n","        Residual Connection with Pre-Norm (Better Stability).\n","        \"\"\"\n","        super(ResidualConnection, self).__init__()\n","        self.norm = LayerNorm(embed_dim)  # Pre-LayerNorm\n","        self.dropout = nn.Dropout(dropout)  # Regularization\n","\n","    def forward(self, x, sublayer):\n","        # Apply Pre-LayerNorm, sublayer, and residual connection\n","        return x + self.dropout(sublayer(self.norm(x)))\n","\n","\n","\n","# âœ… Hyperparameters\n","batch_size = 8\n","seq_len = 128\n","embed_dim = 512\n","hidden_dim = 2048  # FFN hidden dimension\n","num_heads = 8\n","\n","# âœ… Initialize Layers\n","self_attention = MultiHeadSelfAttention(embed_dim, num_heads).to(device)\n","ffn = FeedforwardNetwork(embed_dim, hidden_dim).to(device)\n","residual_connection1 = ResidualConnection(embed_dim).to(device)\n","residual_connection2 = ResidualConnection(embed_dim).to(device)\n","\n","# âœ… Example Input (Random Token Embeddings)\n","input_embeddings = torch.randn(batch_size, seq_len, embed_dim, dtype=torch.float32, device=device)\n","\n","# âœ… Causal Mask (For Decoder)\n","mask = torch.tril(torch.ones(seq_len, seq_len, dtype=torch.float32, device=device)).unsqueeze(0).unsqueeze(0)\n","\n","# âœ… Apply Masked Multi-Head Self-Attention with Residual Connection\n","attention_output = residual_connection1(input_embeddings, self_attention)\n","\n","# âœ… Apply Feedforward Network with Residual Connection\n","ffn_output = residual_connection2(attention_output, ffn)\n","\n","# âœ… Debugging Info\n","print(\"âœ… Input Embeddings Shape:\", input_embeddings.shape)  # Expected: (8, 128, 512)\n","print(\"âœ… Attention Output Shape:\", attention_output.shape)  # Expected: (8, 128, 512)\n","print(\"âœ… FFN Output Shape:\", ffn_output.shape)  # Expected: (8, 128, 512)\n","print(\"âœ… Output dtype:\", ffn_output.dtype)  # Expected: float32"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6BtfaY-BS90A","executionInfo":{"status":"ok","timestamp":1749754156972,"user_tz":-330,"elapsed":53,"user":{"displayName":"Ankit kashyap","userId":"13344809073766364463"}},"outputId":"ff7ddbb0-41a5-46c4-ca55-fccf33ad4203"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Input Embeddings Shape: torch.Size([8, 128, 512])\n","âœ… Attention Output Shape: torch.Size([8, 128, 512])\n","âœ… FFN Output Shape: torch.Size([8, 128, 512])\n","âœ… Output dtype: torch.float32\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","# from MultiHeadSelfAttention.MultiHeadSelfAttention import MultiHeadSelfAttention\n","# from PostNorm.PostNorm import FeedforwardNetwork, LayerNorm, ResidualConnection\n","# from RotaryPositionalEncoding.RotaryPositionalEncoding import RotaryPositionalEncoding\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","\n","\n","\n","\n","# âœ… Layer 1: Sublayers\n","class Layer1(nn.Module):\n","    def __init__(self, embed_dim, hidden_dim, num_heads, dropout=0.1):\n","        super(Layer1, self).__init__()\n","        # Sublayers\n","        self.pre_layer_norm = LayerNorm(embed_dim)  # Pre-LayerNorm\n","        # Remove the rotary_pe argument here as MultiHeadSelfAttention handles it internally\n","        self.self_attention = MultiHeadSelfAttention(embed_dim, num_heads)\n","\n","        self.residual1 = ResidualConnection(embed_dim, dropout)  # Residual Connection 1\n","        self.layer_norm1 = LayerNorm(embed_dim)  # LayerNorm after Self-Attention\n","        self.ffn = FeedforwardNetwork(embed_dim, hidden_dim, dropout)  # Feedforward Network\n","        self.residual2 = ResidualConnection(embed_dim, dropout)  # Residual Connection 2\n","        self.layer_norm2 = LayerNorm(embed_dim)  # LayerNorm after FFN\n","\n","    def forward(self, x):\n","        # Sublayer 1: Self-Attention with Residual Connection\n","        # Note: MultiHeadSelfAttention will apply RoPE internally now\n","        # A causal mask might be needed depending on the model type (e.g., decoder)\n","        # For now, passing None as mask as per previous code blocks. Add mask logic if needed.\n","        attention_output = self.self_attention(self.pre_layer_norm(x)) # Apply pre-norm before attention\n","        x = self.residual1(x, lambda _: attention_output)  # Self-Attention + Residual\n","        # The original code had layer_norm1 here, but with Pre-LN, this is usually after the residual and before the next sublayer's pre-norm\n","\n","        # Sublayer 2: Feedforward Network with Residual Connection\n","        ffn_output = self.ffn(self.layer_norm1(x)) # Apply layer_norm1 (after first residual) before FFN\n","        x = self.residual2(x, lambda _: ffn_output)  # FFN + Residual\n","        # The final layer norm for the block is layer_norm2\n","        x = self.layer_norm2(x)\n","\n","        return x\n","\n","\n","# âœ… Hyperparameters\n","batch_size = 8\n","seq_len = 128\n","embed_dim = 512\n","hidden_dim = 2048  # FFN hidden dimension\n","num_heads = 8\n","\n","# âœ… Initialize Layer 1\n","layer1 = Layer1(embed_dim, hidden_dim, num_heads).to(device)\n","\n","# âœ… Example Input (Random Token Embeddings)\n","input_embeddings = torch.randn(batch_size, seq_len, embed_dim, dtype=torch.float32, device=device)\n","\n","# âœ… Apply Layer 1\n","output = layer1(input_embeddings)\n","\n","# âœ… Debugging Info\n","print(\"âœ… Input Embeddings Shape:\", input_embeddings.shape)  # Expected: (8, 128, 512)\n","print(\"âœ… Layer 1 Output Shape:\", output.shape)  # Expected: (8, 128, 512)\n","print(\"âœ… Output dtype:\", output.dtype)  # Expected: float32"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Ivvf1NES9yu","executionInfo":{"status":"ok","timestamp":1749754157045,"user_tz":-330,"elapsed":68,"user":{"displayName":"Ankit kashyap","userId":"13344809073766364463"}},"outputId":"488e2a87-9d06-4df1-c19c-d2aa84746702"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Input Embeddings Shape: torch.Size([8, 128, 512])\n","âœ… Layer 1 Output Shape: torch.Size([8, 128, 512])\n","âœ… Output dtype: torch.float32\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# âœ… Multi-Head Self-Attention Layer\n","class MultiHeadSelfAttention(nn.Module):\n","    def __init__(self, embed_dim, num_heads):\n","        super(MultiHeadSelfAttention, self).__init__()\n","        self.embed_dim = embed_dim # Store embed_dim as an attribute\n","        self.num_heads = num_heads  # Store num_heads as an attribute\n","        self.head_dim = embed_dim // num_heads\n","\n","        assert self.head_dim * num_heads == embed_dim, \"Embedding dimension must be divisible by number of heads\"\n","\n","        # Fused QKV Projection (Single Linear Layer for Efficiency)\n","        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim, dtype=torch.float32)\n","        self.out_proj = nn.Linear(embed_dim, embed_dim, dtype=torch.float32)\n","\n","    def forward(self, x, mask=None):\n","        batch_size, seq_len, embed_dim = x.shape\n","\n","        # Compute Q, K, V in a single pass\n","        qkv = self.qkv_proj(x).view(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n","        Q, K, V = qkv.unbind(dim=2)  # Split into separate tensors\n","\n","        # Reshape for multi-head attention\n","        Q = Q.transpose(1, 2)  # Shape: (batch, num_heads, seq_len, head_dim)\n","        K = K.transpose(1, 2)\n","        V = V.transpose(1, 2)\n","\n","        # Flash Attention (Optimized Scaled Dot-Product Attention)\n","        if mask is not None:\n","            mask = mask.to(dtype=x.dtype, device=x.device)  # Ensure mask is on the correct device and dtype\n","        output = F.scaled_dot_product_attention(Q, K, V, attn_mask=mask)\n","\n","        # Reshape back to original shape\n","        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n","\n","        # Apply output projection\n","        return self.out_proj(output)\n","\n","# âœ… Chunked Attention (Fixed)\n","class ChunkedAttention(nn.Module):\n","    def __init__(self, embed_dim, num_heads, chunk_size=32):\n","        super(ChunkedAttention, self).__init__()\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads\n","        self.chunk_size = chunk_size\n","        self.self_attention = MultiHeadSelfAttention(embed_dim, num_heads)  # Self-Attention\n","\n","    def forward(self, x):\n","        batch_size, seq_len, embed_dim = x.shape\n","\n","        # Step 1: Pad input if necessary\n","        pad_len = (self.chunk_size - seq_len % self.chunk_size) % self.chunk_size\n","        x = F.pad(x, (0, 0, 0, pad_len))  # Pad along seq_len\n","        seq_len += pad_len\n","\n","        # Step 2: Divide input into chunks\n","        num_chunks = seq_len // self.chunk_size\n","        x = x.view(batch_size, num_chunks, self.chunk_size, embed_dim)  # (batch, num_chunks, chunk_size, embed_dim)\n","\n","        # Step 3: Apply self-attention to each chunk\n","        x = x.reshape(batch_size * num_chunks, self.chunk_size, embed_dim)  # (batch * num_chunks, chunk_size, embed_dim)\n","        x = self.self_attention(x)  # Apply self-attention\n","        x = x.reshape(batch_size, num_chunks, self.chunk_size, embed_dim)  # Reshape back\n","\n","        # Step 4: Combine chunks back into sequence\n","        x = x.reshape(batch_size, seq_len, embed_dim)  # Reshape back\n","\n","        return x[:, :seq_len - pad_len, :]  # Remove padding\n","\n","# âœ… LayerNorm Wrapper\n","class LayerNorm(nn.Module):\n","    def __init__(self, embed_dim):\n","        super(LayerNorm, self).__init__()\n","        self.norm = nn.LayerNorm(embed_dim)\n","\n","    def forward(self, x):\n","        if x.dtype != torch.float32:\n","            x = x.to(torch.float32)  # Ensure input is float32\n","        return self.norm(x)\n","\n","# âœ… Recurrent Memory (Fixed)\n","class RecurrentMemory(nn.Module):\n","    def __init__(self, embed_dim, memory_size, num_heads):  # Add num_heads as a parameter\n","        super(RecurrentMemory, self).__init__()\n","        self.memory_size = memory_size\n","        self.num_heads = num_heads  # Store num_heads as an attribute\n","        self.memory_bank = nn.Parameter(torch.zeros(memory_size, embed_dim, dtype=torch.float32))  # Use float32\n","        self.memory_norm = LayerNorm(embed_dim)  # Normalize memory output\n","        self.memory_attention = nn.MultiheadAttention(embed_dim, num_heads=num_heads, batch_first=True)  # Use num_heads\n","\n","    def forward(self, x):\n","        batch_size, seq_len, embed_dim = x.shape\n","\n","        # Step 1: Memory Read (Multi-query attention)\n","        memory_expanded = self.memory_bank.unsqueeze(0).expand(batch_size, -1, -1)  # (batch, memory_size, embed_dim)\n","        memory_output, _ = self.memory_attention(x, memory_expanded, memory_expanded)  # (batch, seq_len, embed_dim)\n","        memory_output = self.memory_norm(memory_output)  # Normalize memory output\n","\n","        # Step 2: Integrate memory output into main stream\n","        x = x + memory_output  # Add memory output to input\n","\n","        # Step 3: Memory Write (Update memory bank)\n","        self.update_memory(x)\n","\n","        return x\n","\n","    def update_memory(self, x):\n","        with torch.no_grad():\n","            # Aggregate information for update\n","            update_value = x.mean(dim=1).mean(dim=0)\n","\n","            # Create a new memory bank with shifted values\n","            updated_memory_bank = torch.roll(self.memory_bank, shifts=-1, dims=0)\n","            updated_memory_bank[-1] = update_value\n","\n","            # Update the memory bank using a non-in-place operation\n","            self.memory_bank.data = updated_memory_bank  # Assign to .data to avoid in-place operation"],"metadata":{"id":"gcBAumlQS9w7","executionInfo":{"status":"ok","timestamp":1749754157060,"user_tz":-330,"elapsed":12,"user":{"displayName":"Ankit kashyap","userId":"13344809073766364463"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["import torch\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# from MultiHeadSelfAttention.MultiHeadSelfAttention import MultiHeadSelfAttention\n","# from PostNorm.PostNorm import FeedforwardNetwork, LayerNorm, ResidualConnection\n","# from ChunkedAttention.ChunkedAttention import ChunkedAttention, RecurrentMemory # Uncomment this line\n","# from RotaryPositionalEncoding.RotaryPositionalEncoding import RotaryPositionalEncoding\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","class Layer2WithMemory(nn.Module):\n","    def __init__(self, embed_dim, hidden_dim, num_heads, memory_size, dropout=0.1):\n","        super(Layer2WithMemory, self).__init__()\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads\n","        self.memory_size = memory_size\n","\n","        # Sublayers\n","        self.pre_layer_norm = LayerNorm(embed_dim)\n","        # Assuming RotaryPositionalEncoding is also defined or imported elsewhere.\n","        # If not, you might need to uncomment its import too or define it here.\n","        # rope = RotaryPositionalEncoding(embed_dim=embed_dim).to(device) # This instance isn't used directly in this class's __init__\n","        self.self_attention = MultiHeadSelfAttention(embed_dim, num_heads)\n","        self.chunked_attention = ChunkedAttention(embed_dim, num_heads)  # âœ… New\n","        self.recurrent_memory = RecurrentMemory(embed_dim, memory_size, num_heads)  # âœ… New\n","\n","        self.residual1 = ResidualConnection(embed_dim, dropout)\n","        self.layer_norm1 = LayerNorm(embed_dim)\n","\n","        # Memory Module\n","        self.memory_bank = nn.Parameter(torch.zeros(memory_size, embed_dim, dtype=torch.float32))\n","        self.memory_norm = LayerNorm(embed_dim)\n","        self.memory_attention = MultiHeadSelfAttention(embed_dim, num_heads)\n","\n","        # Learnable Memory Update Mechanism\n","        self.memory_gate = nn.Linear(embed_dim, 1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","        # Feedforward Network\n","        self.ffn = FeedforwardNetwork(embed_dim, hidden_dim, dropout)\n","        self.residual2 = ResidualConnection(embed_dim, dropout)\n","        self.layer_norm2 = LayerNorm(embed_dim)\n","\n","    def forward(self, x):\n","        batch_size, seq_len, embed_dim = x.shape\n","        num_heads = self.memory_attention.num_heads\n","        head_dim = embed_dim // num_heads\n","\n","        # === Self-Attention + Chunked Attention + Recurrent Memory ===\n","        # Note: Self-Attention within this layer definition should handle masks if needed.\n","        # If MultiHeadSelfAttention requires a mask, you'll need to add it here.\n","        attn_output = self.self_attention(self.pre_layer_norm(x)) # Apply pre-norm before attention\n","        chunked_output = self.chunked_attention(self.pre_layer_norm(x)) # Apply pre-norm before chunked attention\n","        recurrent_output = self.recurrent_memory(self.pre_layer_norm(x)) # Apply pre-norm before recurrent memory\n","\n","        # Combine all three attentions (average for simplicity; you can try weighted sum too)\n","        combined_attn = (attn_output + chunked_output + recurrent_output) / 3\n","\n","        # The ResidualConnection expects a function as the second argument.\n","        # We are applying the combined_attn output directly, so the lambda\n","        # should just return the combined_attn.\n","        x = self.residual1(x, lambda _: combined_attn)\n","        x = self.layer_norm1(x)\n","\n","        # === Memory Read ===\n","        # Ensure memory_bank has correct device and type\n","        memory_bank = self.memory_bank.unsqueeze(0).expand(batch_size, -1, -1).to(x.device, x.dtype)\n","        memory_bank = memory_bank.view(batch_size, self.memory_size, num_heads, head_dim).permute(0, 2, 1, 3)\n","        Q = x.view(batch_size, seq_len, num_heads, head_dim).permute(0, 2, 1, 3)\n","        # Assuming memory_bank should be treated as K and V for memory attention\n","        memory_output = F.scaled_dot_product_attention(Q, memory_bank, memory_bank)\n","        memory_output = memory_output.permute(0, 2, 1, 3).reshape(batch_size, seq_len, embed_dim)\n","\n","        # Apply memory gate\n","        gate_weight = self.sigmoid(self.memory_gate(x))\n","        memory_output = gate_weight * memory_output\n","        x = x + memory_output # Add memory output as a form of residual\n","\n","        # === Memory Update ===\n","        self.update_memory(x)\n","\n","        # === FFN ===\n","        # Apply layer_norm1 before FFN as per Pre-LN pattern\n","        ffn_output = self.ffn(self.layer_norm1(x))\n","        x = self.residual2(x, lambda _: ffn_output) # Wrap ffn_output in lambda\n","        x = self.layer_norm2(x) # Final layer norm for the block\n","        return x\n","\n","    def update_memory(self, x):\n","        with torch.no_grad():\n","            # Average across sequence length to get a single vector per batch\n","            update_value = x.mean(dim=1) # Shape (batch_size, embed_dim)\n","\n","            # Shift memory bank\n","            updated_memory_bank = torch.roll(self.memory_bank.data, shifts=-1, dims=0)\n","\n","            # Calculate update gate for each sample in the batch\n","            gate = self.sigmoid(self.memory_gate(update_value)) # Shape (batch_size, 1)\n","\n","            # Get the memory slot that is being updated (the oldest slot after rolling)\n","            memory_slot_to_update = updated_memory_bank[0, :].unsqueeze(0).expand_as(update_value) # Shape (batch_size, embed_dim)\n","\n","            # Apply gating to the update value and the current memory slot\n","            # The update_value is what we want to write, gated by `gate`.\n","            # The memory_slot_to_update is the old value, kept by `1 - gate`.\n","            updated_memory_slot_for_batch = (gate * update_value) + (1 - gate) * memory_slot_to_update # Shape (batch_size, embed_dim)\n","\n","            # Average the updated memory slot across the batch to get a single vector\n","            # for the memory bank's slot 0.\n","            updated_memory_bank[0, :] = updated_memory_slot_for_batch.mean(dim=0) # Shape (embed_dim,)\n","\n","            # Update the persistent memory bank parameter data\n","            self.memory_bank.data = updated_memory_bank.data\n","\n","\n","\n","# âœ… Hyperparameters\n","batch_size = 8\n","seq_len = 128\n","embed_dim = 512\n","hidden_dim = 2048  # FFN hidden dimension\n","num_heads = 8\n","memory_size = 100  # Memory bank size\n","\n","# Assume LayerNorm, FeedforwardNetwork, ResidualConnection, MultiHeadSelfAttention, and RotaryPositionalEncoding\n","# are defined or imported in previous cells or scripts.\n","\n","# âœ… Initialize Layer 2 with Memory\n","# Ensure other necessary classes (LayerNorm, FeedforwardNetwork, ResidualConnection, MultiHeadSelfAttention)\n","# are defined or imported before this cell runs.\n","layer2_with_memory = Layer2WithMemory(embed_dim, hidden_dim, num_heads, memory_size).to(device)\n","\n","# âœ… Example Input (Layer 1 ka output)\n","input_embeddings = torch.randn(batch_size, seq_len, embed_dim, dtype=torch.float32, device=device)\n","\n","# âœ… Apply Layer 2 with Memory\n","output = layer2_with_memory(input_embeddings)\n","\n","# âœ… Debugging Info\n","print(\"âœ… Input Embeddings Shape:\", input_embeddings.shape)  # Expected: (8, 128, 512)\n","print(\"âœ… Layer 2 Output Shape:\", output.shape)  # Expected: (8, 128, 512)\n","print(\"âœ… Output dtype:\", output.dtype)  # Expected: float32"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wi5Gcpk5S9vu","executionInfo":{"status":"ok","timestamp":1749754157180,"user_tz":-330,"elapsed":118,"user":{"displayName":"Ankit kashyap","userId":"13344809073766364463"}},"outputId":"4cdf3184-a9b2-4f76-a585-a5764ab8063c"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Input Embeddings Shape: torch.Size([8, 128, 512])\n","âœ… Layer 2 Output Shape: torch.Size([8, 128, 512])\n","âœ… Output dtype: torch.float32\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","\n","\n","\n","class RewardModel(nn.Module):\n","    def __init__(self, embed_dim):\n","        super(RewardModel, self).__init__()\n","        self.linear1 = nn.Linear(embed_dim, 256)  # First linear layer\n","        self.linear2 = nn.Linear(256, 1)  # Second linear layer\n","        self.activation = nn.ReLU()  # Activation function\n","        self.dropout = nn.Dropout(0.1)  # Dropout for regularization\n","\n","    def forward(self, x):\n","        x = self.activation(self.linear1(x))  # Apply first linear layer and activation\n","        x = self.dropout(x)  # Apply dropout\n","        x = self.linear2(x).float()  # Apply second linear layer and ensure float32 output\n","        return x  # Shape: (batch, seq_len, 1)\n","\n","class PPOOptimizer:\n","    def __init__(self, model, reward_model, lr=1e-4, gamma=0.99, clip_epsilon=0.2, entropy_coef=0.01):\n","        self.model = model\n","        self.reward_model = reward_model\n","        # Ensure optimizer is created *after* model parameters are on the correct device\n","        self.optimizer = optim.Adam(model.parameters(), lr=lr)  # Adam optimizer\n","        self.gamma = gamma  # Discount factor\n","        self.clip_epsilon = clip_epsilon  # Clipping parameter for PPO\n","        self.entropy_coef = entropy_coef  # Entropy coefficient\n","\n","    def compute_advantages(self, rewards, values):\n","        \"\"\"\n","        Compute advantages. Assuming rewards are per-batch mean rewards\n","        and values are per-batch values of the last state.\n","        \"\"\"\n","        # Simple advantage: Reward - Value\n","        # Ensure shapes are compatible. rewards is (batch_size, 1), values is (batch_size,)\n","        # Unsqueeze values to (batch_size, 1) for element-wise subtraction\n","        advantages = rewards - values.unsqueeze(1)\n","        return advantages\n","\n","    def update(self, states, actions, rewards, old_log_probs, values):\n","        print(\"ðŸ” PPO Update Start\")\n","\n","        # Compute advantages\n","        advantages = self.compute_advantages(rewards, values)\n","\n","        # Safe normalization\n","        adv_mean = advantages.mean()\n","        adv_std = advantages.std()\n","        if not torch.isfinite(adv_std) or adv_std < 1e-6:\n","            print(\"âš ï¸ Unstable advantage std detected, skipping PPO update.\")\n","            return torch.tensor(0.0, device=states.device)\n","        advantages = (advantages - adv_mean) / (adv_std + 1e-8)\n","\n","        print(f\"ðŸ”¹ Advantages: {advantages.shape}\")\n","\n","        # Forward pass again\n","        output, new_log_probs, _, new_values, _ = self.model(states, actions)\n","\n","        # Clamp log probs to prevent exp overflow\n","        new_log_probs = torch.clamp(new_log_probs, min=-20, max=0)\n","        old_log_probs = torch.clamp(old_log_probs, min=-20, max=0)\n","\n","        # Calculate returns\n","        returns = advantages.squeeze(1) + values  # shape: (batch_size,)\n","        print(f\"âœ… Log Probs: {new_log_probs.shape}, âœ… Values: {new_values.shape}\")\n","        print(f\"ðŸ”„ Returns shape: {returns.shape}\")\n","\n","        # PPO ratio\n","        ratio = torch.exp(new_log_probs - old_log_probs)\n","        clipped_ratio = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon)\n","        policy_loss = -torch.min(ratio.unsqueeze(1) * advantages,\n","                                 clipped_ratio.unsqueeze(1) * advantages).mean()\n","\n","        # Value loss\n","        value_loss = F.mse_loss(new_values, returns)\n","\n","        # Total loss\n","        loss = policy_loss + 0.5 * value_loss\n","\n","        # Check for NaNs or Infs before backward\n","        if not torch.isfinite(loss):\n","            print(\"ðŸš¨ Loss is NaN/Inf. Skipping PPO update.\")\n","            return torch.tensor(0.0, device=states.device)\n","\n","        print(f\"ðŸ“‰ Loss: {loss.item()}\")\n","        print(\"âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\")\n","\n","        return loss\n","\n","\n","# âœ… Hyperparameters\n","batch_size = 8\n","seq_len = 128\n","embed_dim = 512\n","\n","# âœ… Initialize Reward Model\n","reward_model = RewardModel(embed_dim).to(device)\n","\n","# âœ… Example Input (Random Token Embeddings)\n","input_embeddings = torch.randn(batch_size, seq_len, embed_dim, dtype=torch.float32, device=device)\n","\n","# âœ… Apply Reward Model\n","rewards = reward_model(input_embeddings)\n","\n","# âœ… Debugging Info\n","print(\"âœ… Input Embeddings Shape:\", input_embeddings.shape)  # Expected: (8, 128, 512)\n","print(\"âœ… Rewards Shape:\", rewards.shape)  # Expected: (8, 128, 1)\n","print(\"âœ… Rewards dtype:\", rewards.dtype)  # Expected: float32"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iBo4dVwFS9uG","executionInfo":{"status":"ok","timestamp":1749754157263,"user_tz":-330,"elapsed":68,"user":{"displayName":"Ankit kashyap","userId":"13344809073766364463"}},"outputId":"766d37e8-62f4-463d-836b-7a92e6e38031"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Input Embeddings Shape: torch.Size([8, 128, 512])\n","âœ… Rewards Shape: torch.Size([8, 128, 1])\n","âœ… Rewards dtype: torch.float32\n"]}]},{"cell_type":"code","source":["import torch\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# from MultiHeadSelfAttention.MultiHeadSelfAttention import MultiHeadSelfAttention,LayerNorm,ResidualConnection\n","# from PostNorm.PostNorm import LayerNorm,ResidualConnection,FeedforwardNetwork\n","# from ChunkedAttention.ChunkedAttention import ChunkedAttention,RecurrentMemory\n","# from RotaryPositionalEncoding.RotaryPositionalEncoding import RotaryPositionalEncoding\n","\n","\n","# Set device\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# Set random seed for reproducibility\n","torch.manual_seed(42)\n","\n","\n","class Layer3WithContextAndRL(nn.Module):\n","    def __init__(self, embed_dim, hidden_dim, num_heads, memory_size, dropout=0.1, num_actions=10):\n","        super(Layer3WithContextAndRL, self).__init__()\n","        self.embed_dim = embed_dim # Store embed_dim as an attribute\n","        # Sublayers\n","        self.pre_layer_norm = LayerNorm(embed_dim)  # Pre-LayerNorm\n","        # Remove the direct initialization and passing of rope here\n","        # rope = RotaryPositionalEncoding(embed_dim=embed_dim).to(device)\n","        # MultiHeadSelfAttention in ipython-input-13-837d8f2a7177 handled RoPE internally based on head_dim\n","        # The MultiHeadSelfAttention imported/defined in a previous cell (ipython-input-19-363be42504bd)\n","        # also handles RoPE internally.\n","        # Initialize MultiHeadSelfAttention without the rotary_pe argument\n","        self.self_attention = MultiHeadSelfAttention(embed_dim, num_heads)\n","\n","        self.residual1 = ResidualConnection(embed_dim, dropout)  # Residual Connection 1\n","        self.layer_norm1 = LayerNorm(embed_dim)  # LayerNorm after Self-Attention\n","\n","        # Context Handling\n","        self.chunked_attention = ChunkedAttention(embed_dim, num_heads)  # Chunked Attention\n","        # Ensure RecurrentMemory also matches its defined __init__ signature (takes embed_dim, memory_size, num_heads)\n","        self.recurrent_memory = RecurrentMemory(embed_dim, memory_size, num_heads)  # Recurrent Memory\n","\n","        # Feedforward Network\n","        self.ffn = FeedforwardNetwork(embed_dim, hidden_dim, dropout)  # FFN\n","        self.residual2 = ResidualConnection(embed_dim, dropout)  # Residual Connection 2\n","        self.layer_norm2 = LayerNorm(embed_dim)  # LayerNorm after FFN\n","\n","        # RL Integration\n","        # Reward Model is now separate, passed during PPO initialization\n","        self.reward_model = nn.Linear(embed_dim, 1)  # Reward Model instance\n","        self.policy_network = nn.Linear(embed_dim, num_actions)  # Policy Network\n","        self.value_network = nn.Linear(embed_dim, 1)  # Value Network\n","\n","        # PPO Optimizer will be initialized outside this module\n","        # self.ppo_optimizer = PPOOptimizer(model=self, reward_model=self.reward_model)  # Initialize PPO Optimizer\n","\n","    def forward(self, x, actions=None):\n","        # Sublayer 1: Self-Attention with Residual Connection\n","        # Ensure MultiHeadSelfAttention forward method is compatible with ResidualConnection's expected input\n","        # ResidualConnection calls self.self_attention(self.norm(x))\n","        # The self_attention forward method in ipython-input-19-363be42504bd takes x and optional mask\n","        # The Layer3WithContextAndRL forward currently does not pass a mask to self_attention\n","        # If a causal mask is needed for this layer, add mask creation and passing here.\n","        x = self.residual1(x, self.self_attention) # residual1 passes normalized x to self_attention\n","        x = self.layer_norm1(x)\n","\n","        # Context Handling\n","        # Check ChunkedAttention and RecurrentMemory forward methods to ensure they take the correct input (x)\n","        # ChunkedAttention and RecurrentMemory from ipython-input-19-363be42504bd take x as input\n","        x = self.chunked_attention(x)\n","        x = self.recurrent_memory(x)\n","\n","        # Sublayer 2: Feedforward Network with Residual Connection\n","        # residual2 calls self.ffn(self.norm(x))\n","        # FeedforwardNetwork from ipython-input-15-45a01112038a takes x as input\n","        x = self.residual2(x, self.ffn) # residual2 passes normalized x to ffn\n","        x = self.layer_norm2(x)\n","\n","        # RL Integration: Compute Rewards (part of the forward pass for gradient purposes)\n","        # Note: The reward_model instance used here must be the same one passed to PPOOptimizer\n","        # The reward_model forward method expects input_embeddings which is (batch, seq_len, embed_dim)\n","        # and outputs (batch, seq_len, 1)\n","        rewards = self.reward_model(x)  # Shape: (batch_size, seq_len, 1)\n","\n","        # Policy Network (Action Selection)\n","        last_hidden_state = x[:, -1, :]  # Use the last hidden state for action selection\n","        logits = self.policy_network(last_hidden_state) # policy_network expects (batch, embed_dim), outputs (batch, num_actions)\n","        probs = F.softmax(logits, dim=-1) # probs is (batch, num_actions)\n","\n","        # If actions are not provided, sample from policy\n","        # If actions *are* provided (during PPO update), use them\n","        if actions is None:\n","            # Sample actions when actions are not provided (first forward pass)\n","            # torch.multinomial expects input probs (batch, num_actions) and returns (batch, num_samples)\n","            actions = torch.multinomial(probs, num_samples=1).squeeze(-1) # actions is (batch,)\n","\n","\n","        # Compute log probs for selected actions\n","        # probs.gather expects index tensor (batch, 1) if dim=-1\n","        # actions is (batch,), unsqueeze to (batch, 1)\n","        new_log_probs = torch.log(probs.gather(-1, actions.unsqueeze(-1))).squeeze(-1) # new_log_probs is (batch,)\n","\n","\n","        # Value Network (State Value Estimation)\n","        # value_network expects (batch, embed_dim), outputs (batch, 1)\n","        # squeeze(-1) makes it (batch,)\n","        values = self.value_network(last_hidden_state).squeeze(-1) # values is (batch,)\n","\n","        return x, new_log_probs, actions, values, rewards  # Return all five values"],"metadata":{"id":"aIwHGN6zS9qa","executionInfo":{"status":"ok","timestamp":1749754157323,"user_tz":-330,"elapsed":61,"user":{"displayName":"Ankit kashyap","userId":"13344809073766364463"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["import os\n","import torch\n","from torch.amp import GradScaler, autocast\n","from torch.utils.data import Dataset, DataLoader\n","import json\n","from pathlib import Path\n","import time\n","\n","# from Layer1.Layer1 import Layer1\n","# from Layer2WithMemory.Layer2WithMemory import Layer2WithMemory\n","# from TokenEmbeddings.TokenEmbeddings import TokenEmbedding\n","# from Tokenization.tokenization import TokenizationLayer\n","# from Layer3WithContextAndRL.Layer3WithContextAndRL import Layer3WithContextAndRL\n","# from PPOOptimizer.PPOOptimizer import PPOOptimizer\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n","torch.manual_seed(42)\n","print(f\"Using {device} for training ðŸ”¥\")\n","\n","# -------- Dataset --------\n","# -------- Dataset --------\n","class BhaiDataset(Dataset):\n","    def __init__(self, file_path, max_samples=None):\n","        self.file_path = Path(file_path)\n","        self.max_samples = max_samples\n","        self.data = self._load_data()\n","\n","    def _load_data(self):\n","        if self.file_path.suffix == '.txt':\n","            with open(self.file_path, 'r', encoding='utf-8', errors='replace') as f:\n","                lines = [self._clean(line) for line in f if line.strip()]\n","        elif self.file_path.suffix == '.json':\n","            with open(self.file_path, 'r', encoding='utf-8') as f:\n","                data = json.load(f)\n","                lines = self._parse_json(data)\n","        else:\n","            raise ValueError(\"Only .txt or .json files supported!\")\n","\n","        if self.max_samples:\n","            lines = lines[:self.max_samples]\n","        return lines\n","\n","    def _clean(self, text):\n","        return ''.join(c for c in text if c not in {'\\x00', '\\ufffd', 'ï¿½', '\\r'}).strip()\n","\n","    def _parse_json(self, data):\n","        texts = []\n","        if isinstance(data, list):\n","            for item in data:\n","                if 'text' in item:\n","                    texts.append(self._clean(item['text']))\n","                elif 'content' in item:\n","                    texts.append(self._clean(item['content']))\n","        elif isinstance(data, dict):\n","            for v in data.values():\n","                if isinstance(v, str):\n","                    texts.append(self._clean(v))\n","        return texts\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        return self.data[idx]\n","\n","# -------- Training Loop --------\n","def bhai_trainer(dataset_path, epochs=5, batch_size=8, max_seq_len=128, max_samples=50000):\n","    dataset = BhaiDataset(dataset_path, max_samples=max_samples)\n","    print(f\"ðŸ“¦ Loaded {len(dataset)} samples out of total with limit {max_samples}\")\n","\n","    tokenizer = TokenizationLayer()\n","\n","    def collate_fn(batch):\n","        tokens = tokenizer.tokenize(batch, max_length=max_seq_len)\n","        return tokens.to(device)\n","\n","    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=0)\n","    print(f\"ðŸ”¢ Batches per epoch: {len(dataloader)} with batch size {batch_size}\")\n","\n","    embed_dim = 256\n","    hidden_dim = 1024\n","    num_heads = 8\n","    memory_size = 100\n","\n","    embedder = TokenEmbedding(tokenizer.vocab_size, embed_dim=embed_dim).to(device)\n","    layer1 = Layer1(embed_dim, hidden_dim, num_heads).to(device)\n","    layer2 = Layer2WithMemory(embed_dim, hidden_dim, num_heads, memory_size).to(device)\n","    layer3 = Layer3WithContextAndRL(embed_dim, hidden_dim, num_heads, memory_size).to(device)\n","\n","    ppo_optimizer = PPOOptimizer(model=layer3, reward_model=None)\n","    optimizer = ppo_optimizer.optimizer\n","    scaler = GradScaler()\n","\n","    for epoch in range(epochs):\n","        epoch_start = time.time()\n","        for batch_idx, inputs in enumerate(dataloader):\n","            batch_start = time.time()\n","            optimizer.zero_grad()\n","\n","            try:\n","                with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n","                    emb = embedder(inputs)\n","                    l1_out = layer1(emb)\n","                    l2_out = layer2(l1_out)\n","\n","                    out, old_log_probs, actions, values, rewards = layer3(l2_out, actions=None)\n","\n","                    states = l2_out.detach().to(torch.float32)\n","                    actions = actions.detach()\n","                    old_log_probs = old_log_probs.detach().to(torch.float32)\n","                    values = values.detach().to(torch.float32)\n","                    rewards = rewards.mean(dim=1).detach().to(torch.float32)\n","\n","                    loss = ppo_optimizer.update(\n","                        states=states,\n","                        actions=actions,\n","                        rewards=rewards,\n","                        old_log_probs=old_log_probs,\n","                        values=values\n","                    )\n","\n","                if not torch.isfinite(loss) or not loss.requires_grad:\n","                    raise ValueError(\"Loss is NaN/Inf or does not require grad.\")\n","\n","                scaler.scale(loss).backward()\n","                scaler.unscale_(optimizer)\n","                torch.nn.utils.clip_grad_norm_(layer3.parameters(), max_norm=0.5)\n","                scaler.step(optimizer)\n","                scaler.update()\n","\n","                if batch_idx % 10 == 0:\n","                    gpu_mem = torch.cuda.memory_allocated() // 1024 ** 2 if torch.cuda.is_available() else 0\n","                    batch_time = time.time() - batch_start\n","                    print(f\"Epoch {epoch + 1} | Batch {batch_idx} | Loss: {loss.item():.4f} | Reward: {rewards.mean().item():.4f} | Value: {values.mean().item():.4f} | GPU: {gpu_mem}MB | Time/batch: {batch_time:.2f}s\")\n","\n","            except Exception as e:\n","                print(f\"ðŸš¨ Skipping batch {batch_idx} due to error: {e}\")\n","                continue\n","\n","        epoch_time = time.time() - epoch_start\n","        print(f\"âœ… Epoch {epoch + 1} done in {epoch_time:.2f} seconds.\")\n","\n","        try:\n","            torch.save({\n","                'epoch': epoch + 1,\n","                'loss': loss.item() if torch.isfinite(loss) else None,\n","                'layer1_state_dict': layer1.state_dict(),\n","                'layer2_state_dict': layer2.state_dict(),\n","                'layer3_state_dict': layer3.state_dict(),\n","                'embedder_state_dict': embedder.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'scaler_state_dict': scaler.state_dict()\n","            }, f\"bhai_llm_epoch_{epoch + 1}.pt\")\n","            print(f\"âœ… Model checkpoint saved: bhai_llm_epoch_{epoch + 1}.pt\")\n","        except Exception as e:\n","            print(f\"ðŸš¨ Error saving checkpoint: {e}\")\n","\n","# -------- Main --------\n","if __name__ == \"__main__\":\n","    print(\"ðŸš€ Starting training...\")\n","\n","    dataset_path = \"/content/conversation_chatgpt.txt\"\n","    epochs = 5\n","    batch_size = 8\n","\n","    if not Path(dataset_path).exists():\n","        print(f\"ðŸš¨ Dataset file not found: {dataset_path}\")\n","    else:\n","        bhai_trainer(dataset_path, epochs, batch_size, max_samples=50000)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IqNQ9JWYS9n_","executionInfo":{"status":"ok","timestamp":1749754784766,"user_tz":-330,"elapsed":627441,"user":{"displayName":"Ankit kashyap","userId":"13344809073766364463"}},"outputId":"91cca905-fdff-49a4-af8f-f4dda9ef6aa2"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","ðŸ“‰ Loss: 0.4235021471977234\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5430 | Loss: 0.4235 | Reward: 0.6964 | Value: 6.2593 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4354788661003113\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41691529750823975\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.425884485244751\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4337064027786255\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41498106718063354\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4243946373462677\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4287847876548767\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4249539375305176\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.44069454073905945\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42405450344085693\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5440 | Loss: 0.4241 | Reward: 0.6998 | Value: 6.2788 | GPU: 1186MB | Time/batch: 0.03s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4182913899421692\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4310609996318817\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41892915964126587\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.44123274087905884\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41928383708000183\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4167858958244324\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4255475401878357\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4233121871948242\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4293617904186249\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4195897877216339\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5450 | Loss: 0.4196 | Reward: 0.7001 | Value: 6.2705 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42291539907455444\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42100512981414795\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42915108799934387\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42464232444763184\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42394763231277466\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4319613575935364\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.40571799874305725\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4165160655975342\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41880011558532715\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4240344762802124\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5460 | Loss: 0.4240 | Reward: 0.6975 | Value: 6.1929 | GPU: 1186MB | Time/batch: 0.05s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4289659857749939\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42433756589889526\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41730988025665283\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4246467649936676\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42135751247406006\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42826560139656067\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42343708872795105\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42541971802711487\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4228912889957428\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42481282353401184\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5470 | Loss: 0.4248 | Reward: 0.6960 | Value: 6.1685 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4245220422744751\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4092721939086914\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43511977791786194\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4198229908943176\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43429267406463623\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4288356304168701\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4283791780471802\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4390859603881836\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4175379276275635\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.417356014251709\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5480 | Loss: 0.4174 | Reward: 0.6973 | Value: 6.2769 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4273335933685303\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4167823791503906\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4174647629261017\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4168209433555603\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43022435903549194\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41610705852508545\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.413033664226532\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4180360436439514\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4335208833217621\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43216070532798767\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5490 | Loss: 0.4322 | Reward: 0.7013 | Value: 6.3569 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41638612747192383\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4263063371181488\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41018569469451904\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4261389970779419\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4282034933567047\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42587292194366455\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43353039026260376\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43067991733551025\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42844051122665405\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4255850315093994\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5500 | Loss: 0.4256 | Reward: 0.7079 | Value: 6.4253 | GPU: 1186MB | Time/batch: 0.05s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4287501573562622\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41928431391716003\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4160293936729431\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42491796612739563\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4317290782928467\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41936635971069336\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41976040601730347\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4281134605407715\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4221969246864319\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4221050441265106\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5510 | Loss: 0.4221 | Reward: 0.7084 | Value: 6.4287 | GPU: 1186MB | Time/batch: 0.03s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42641085386276245\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43471992015838623\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42324620485305786\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4170321524143219\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41932085156440735\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.431916743516922\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41788041591644287\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4212827980518341\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4205816686153412\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4113922715187073\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5520 | Loss: 0.4114 | Reward: 0.7143 | Value: 6.3613 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4317549467086792\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4212110638618469\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41599053144454956\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4282732903957367\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4137069284915924\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4221841096878052\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42456740140914917\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4251552224159241\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4306221902370453\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43378746509552\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5530 | Loss: 0.4338 | Reward: 0.7167 | Value: 6.2480 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4199375510215759\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4282532334327698\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42842140793800354\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41720128059387207\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4231478273868561\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4230358302593231\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4308686852455139\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4171033799648285\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4242858588695526\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4267648458480835\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5540 | Loss: 0.4268 | Reward: 0.7220 | Value: 6.1318 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43485552072525024\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4227953553199768\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4182940423488617\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4124557673931122\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41473856568336487\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42001956701278687\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4231222867965698\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42939293384552\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43138131499290466\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41923001408576965\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5550 | Loss: 0.4192 | Reward: 0.7264 | Value: 6.0605 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4284038543701172\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42734014987945557\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42889881134033203\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4282674193382263\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41969090700149536\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4247499108314514\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42127299308776855\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43222713470458984\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41220542788505554\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43569129705429077\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5560 | Loss: 0.4357 | Reward: 0.7276 | Value: 6.0552 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.407300740480423\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41469621658325195\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42080336809158325\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42496734857559204\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4225088655948639\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4066275656223297\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4270995855331421\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42024871706962585\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42403316497802734\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4130916893482208\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5570 | Loss: 0.4131 | Reward: 0.7289 | Value: 5.9976 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4203915596008301\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43202081322669983\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.430107057094574\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4188538193702698\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4226360023021698\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42886751890182495\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4263022840023041\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42835524678230286\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4232616424560547\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4175824224948883\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5580 | Loss: 0.4176 | Reward: 0.7343 | Value: 5.9590 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4201667904853821\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41826409101486206\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4247044026851654\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4247453808784485\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.40899020433425903\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41993024945259094\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4142676889896393\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4374582767486572\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4119086265563965\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4223206341266632\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5590 | Loss: 0.4223 | Reward: 0.7332 | Value: 6.1133 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42545461654663086\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4303150177001953\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41482269763946533\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.427203893661499\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43629568815231323\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4289324879646301\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41662514209747314\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.424963116645813\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4150906205177307\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42194411158561707\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5600 | Loss: 0.4219 | Reward: 0.7371 | Value: 6.1753 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4200615882873535\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4209054708480835\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4188258647918701\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4229810833930969\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4284999966621399\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42255234718322754\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42145270109176636\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4185762405395508\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41217052936553955\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41924694180488586\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5610 | Loss: 0.4192 | Reward: 0.7429 | Value: 6.1719 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42180758714675903\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41086238622665405\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4120059609413147\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.40919607877731323\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43612492084503174\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4276087284088135\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4266429543495178\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42529046535491943\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42407408356666565\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4265426993370056\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5620 | Loss: 0.4265 | Reward: 0.7478 | Value: 6.1421 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4126918315887451\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41742515563964844\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41036975383758545\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4184136688709259\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42000168561935425\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.44013845920562744\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4396440386772156\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.425919771194458\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4254533350467682\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41821765899658203\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5630 | Loss: 0.4182 | Reward: 0.7501 | Value: 6.1479 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41770392656326294\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4183371663093567\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42711442708969116\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4181039035320282\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.418440043926239\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4251139163970947\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4228893220424652\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42793887853622437\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43212610483169556\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42643555998802185\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5640 | Loss: 0.4264 | Reward: 0.7548 | Value: 6.1484 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4420756995677948\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4220739006996155\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41874468326568604\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41883644461631775\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42389166355133057\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4297022819519043\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4172407388687134\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.425082802772522\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41351816058158875\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43022966384887695\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5650 | Loss: 0.4302 | Reward: 0.7494 | Value: 6.0933 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4256778955459595\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4259334206581116\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4299789071083069\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4250945448875427\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42623162269592285\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4163983464241028\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43124184012413025\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4214857220649719\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4221068024635315\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42841440439224243\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5660 | Loss: 0.4284 | Reward: 0.7452 | Value: 6.0884 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42524266242980957\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4265854060649872\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4368816316127777\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4299626052379608\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43092814087867737\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4291096329689026\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4218901991844177\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42536795139312744\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4275785982608795\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42498835921287537\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5670 | Loss: 0.4250 | Reward: 0.7429 | Value: 6.0630 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4202505946159363\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4187656342983246\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4357028007507324\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42570582032203674\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4276619553565979\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41349971294403076\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4260079264640808\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41985827684402466\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.423461377620697\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41946786642074585\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5680 | Loss: 0.4195 | Reward: 0.7443 | Value: 6.0142 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4255005121231079\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4179955720901489\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4335629343986511\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41541028022766113\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42274928092956543\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42779964208602905\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41335833072662354\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4171697497367859\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4295392632484436\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41864249110221863\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5690 | Loss: 0.4186 | Reward: 0.7450 | Value: 6.0605 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4183846712112427\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43366003036499023\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4056752920150757\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41995373368263245\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4274289608001709\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4248664677143097\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4148351550102234\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42540109157562256\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4229360818862915\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42202141880989075\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5700 | Loss: 0.4220 | Reward: 0.7483 | Value: 6.1426 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42225077748298645\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4258580207824707\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42690908908843994\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4299312233924866\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42725682258605957\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42679721117019653\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4319366216659546\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4252106249332428\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42175376415252686\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43412014842033386\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5710 | Loss: 0.4341 | Reward: 0.7525 | Value: 6.2314 | GPU: 1186MB | Time/batch: 0.04s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4281371831893921\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42812153697013855\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4252001643180847\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4248915910720825\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4177309572696686\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42251020669937134\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4210449755191803\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4276043772697449\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42448756098747253\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4251773953437805\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5720 | Loss: 0.4252 | Reward: 0.7529 | Value: 6.3193 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41131502389907837\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4094908833503723\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.421073853969574\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42786723375320435\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43474239110946655\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42412760853767395\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4341527223587036\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41578853130340576\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4253118634223938\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43678539991378784\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5730 | Loss: 0.4368 | Reward: 0.7555 | Value: 6.3408 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42605483531951904\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4115886390209198\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41525036096572876\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4420583248138428\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42822742462158203\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43375006318092346\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.419583797454834\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42257052659988403\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4219983220100403\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42679426074028015\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5740 | Loss: 0.4268 | Reward: 0.7557 | Value: 6.3604 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4315968155860901\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4194265604019165\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4220617711544037\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4211837947368622\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42329439520835876\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4256179928779602\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.40885627269744873\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4196717441082001\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42120394110679626\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4252687394618988\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5750 | Loss: 0.4253 | Reward: 0.7623 | Value: 6.3599 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4256467819213867\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4281892478466034\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42828285694122314\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42332732677459717\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42704135179519653\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4251877963542938\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4215339422225952\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4307810664176941\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42379993200302124\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42070215940475464\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5760 | Loss: 0.4207 | Reward: 0.7639 | Value: 6.3877 | GPU: 1186MB | Time/batch: 0.04s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4230335056781769\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41574811935424805\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43790507316589355\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42585641145706177\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4288511276245117\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43295538425445557\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42555874586105347\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41361987590789795\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42673033475875854\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4172075390815735\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5770 | Loss: 0.4172 | Reward: 0.7628 | Value: 6.3882 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42221805453300476\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42116808891296387\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42203471064567566\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4126499891281128\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.429660826921463\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4237419664859772\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4237367510795593\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4235582649707794\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4267347455024719\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4307863116264343\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5780 | Loss: 0.4308 | Reward: 0.7654 | Value: 6.3984 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42962002754211426\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42226508259773254\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4227619171142578\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4312167167663574\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4245430827140808\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.40708309412002563\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4327795207500458\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4168374538421631\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42603132128715515\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42742982506752014\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5790 | Loss: 0.4274 | Reward: 0.7661 | Value: 6.3823 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42487674951553345\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42319685220718384\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4180677533149719\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4218488931655884\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41604357957839966\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42713069915771484\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4240191578865051\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42252352833747864\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4209548830986023\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4257871210575104\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5800 | Loss: 0.4258 | Reward: 0.7594 | Value: 6.4155 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4209315776824951\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42008474469184875\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4302322268486023\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4263341724872589\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4187003970146179\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4296850562095642\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4252702593803406\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4279026985168457\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42037808895111084\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4293745160102844\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5810 | Loss: 0.4294 | Reward: 0.7578 | Value: 6.4878 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4131183922290802\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41553083062171936\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4339465796947479\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4154013693332672\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.428598552942276\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4233158826828003\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4315929710865021\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43006259202957153\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43095117807388306\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41973745822906494\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5820 | Loss: 0.4197 | Reward: 0.7629 | Value: 6.4800 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4297126531600952\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42269167304039\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4259260892868042\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4223567843437195\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41032516956329346\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42503833770751953\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4332005977630615\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41052472591400146\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4325084686279297\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4219198226928711\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5830 | Loss: 0.4219 | Reward: 0.7652 | Value: 6.4580 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4181942343711853\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43247100710868835\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43317198753356934\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4197944700717926\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41649162769317627\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42729651927948\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4267653226852417\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4226889908313751\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4145890474319458\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42191803455352783\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5840 | Loss: 0.4219 | Reward: 0.7662 | Value: 6.4795 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43091046810150146\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4221215844154358\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4298851490020752\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43352532386779785\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42271289229393005\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41902342438697815\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42882516980171204\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41745394468307495\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4253905415534973\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42904627323150635\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5850 | Loss: 0.4290 | Reward: 0.7695 | Value: 6.4434 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4200480878353119\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4268175959587097\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4079549312591553\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42016080021858215\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4241141974925995\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41981005668640137\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4350724220275879\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41930338740348816\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41083645820617676\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42346271872520447\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5860 | Loss: 0.4235 | Reward: 0.7718 | Value: 6.4121 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.422906756401062\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4264081120491028\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4208183288574219\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42289096117019653\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4095575213432312\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4279426038265228\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4201137125492096\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4257146120071411\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4148021638393402\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4192414879798889\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5870 | Loss: 0.4192 | Reward: 0.7709 | Value: 6.3159 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43132057785987854\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42654627561569214\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4158168435096741\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4201432168483734\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.40049809217453003\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.425819993019104\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4182741343975067\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4264805018901825\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4191195070743561\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4248419404029846\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5880 | Loss: 0.4248 | Reward: 0.7729 | Value: 6.1782 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4201194941997528\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4187239408493042\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4260918200016022\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4227273464202881\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42652958631515503\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4269707202911377\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.429779589176178\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41713228821754456\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42446765303611755\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42521360516548157\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5890 | Loss: 0.4252 | Reward: 0.7741 | Value: 6.1880 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43229636549949646\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43314608931541443\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4260712265968323\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4192659258842468\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4240764081478119\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43273526430130005\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4198313355445862\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41456496715545654\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4235166013240814\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4224684536457062\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5900 | Loss: 0.4225 | Reward: 0.7753 | Value: 6.2158 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4110948145389557\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4217056632041931\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4284595549106598\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4247872531414032\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43200454115867615\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4276057481765747\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43582144379615784\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4215899705886841\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4068504571914673\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42057323455810547\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5910 | Loss: 0.4206 | Reward: 0.7784 | Value: 6.2031 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4378282427787781\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4193097651004791\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4284031391143799\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4178270697593689\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4180242419242859\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42840576171875\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42814767360687256\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41957956552505493\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4215184450149536\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4222666025161743\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5920 | Loss: 0.4223 | Reward: 0.7786 | Value: 6.2070 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4187866151332855\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41842663288116455\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4276354908943176\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41118964552879333\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42407798767089844\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4264054596424103\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4220585525035858\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4186158776283264\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4214096665382385\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4267721474170685\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5930 | Loss: 0.4268 | Reward: 0.7847 | Value: 6.1929 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42839914560317993\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42221254110336304\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4060296416282654\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4198271334171295\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4209743142127991\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4260667860507965\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4349536597728729\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42389482259750366\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41465550661087036\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42364799976348877\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5940 | Loss: 0.4236 | Reward: 0.7902 | Value: 6.1270 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43160688877105713\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41932302713394165\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4210951626300812\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.422603964805603\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43024882674217224\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4272059500217438\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41662776470184326\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4350580871105194\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42210403084754944\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4418680667877197\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5950 | Loss: 0.4419 | Reward: 0.7947 | Value: 6.1362 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4309437870979309\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4336795210838318\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42296865582466125\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42334020137786865\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41242170333862305\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4206561744213104\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4226762652397156\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42205673456192017\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42567193508148193\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4364025294780731\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5960 | Loss: 0.4364 | Reward: 0.7958 | Value: 6.1245 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42866137623786926\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42067304253578186\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41733184456825256\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4190687835216522\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4257854223251343\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4236937165260315\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4326505661010742\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43171292543411255\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4229915142059326\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43349558115005493\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5970 | Loss: 0.4335 | Reward: 0.7983 | Value: 6.1108 | GPU: 1186MB | Time/batch: 0.04s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4264042377471924\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41234469413757324\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43225616216659546\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43079936504364014\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42451587319374084\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42144814133644104\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41653943061828613\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42598626017570496\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43140527606010437\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4219896197319031\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5980 | Loss: 0.4220 | Reward: 0.7972 | Value: 6.1382 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4253711998462677\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.433352530002594\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42375701665878296\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4187040627002716\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42410534620285034\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4223615229129791\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4173639416694641\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4224283695220947\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43396374583244324\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4124319851398468\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 5990 | Loss: 0.4124 | Reward: 0.7916 | Value: 6.2007 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4293380379676819\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4340212941169739\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4170370399951935\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42144498229026794\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42370980978012085\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43610918521881104\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.412993848323822\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4279296100139618\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42916297912597656\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42592620849609375\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 6000 | Loss: 0.4259 | Reward: 0.7894 | Value: 6.2456 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4188641309738159\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42660820484161377\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42605215311050415\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4127601981163025\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4319005012512207\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4131008982658386\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42532119154930115\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4253704845905304\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.413329541683197\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42409706115722656\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 6010 | Loss: 0.4241 | Reward: 0.7917 | Value: 6.2280 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4226362705230713\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42497920989990234\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.429054319858551\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41388148069381714\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43182361125946045\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42354437708854675\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42476674914360046\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4206922948360443\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4213809072971344\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42288053035736084\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 6020 | Loss: 0.4229 | Reward: 0.7947 | Value: 6.1997 | GPU: 1186MB | Time/batch: 0.04s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4272755980491638\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42850929498672485\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42023661732673645\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41524943709373474\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4320768415927887\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41582006216049194\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4196963608264923\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4205613136291504\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4263541102409363\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42301812767982483\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 6030 | Loss: 0.4230 | Reward: 0.8007 | Value: 6.1753 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4255948066711426\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42894020676612854\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4419083595275879\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4309292435646057\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4296850860118866\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43380919098854065\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4227313995361328\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43078887462615967\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42385244369506836\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4280586242675781\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 6040 | Loss: 0.4281 | Reward: 0.8015 | Value: 6.1777 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4147592782974243\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4195370078086853\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4158174991607666\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42112886905670166\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4172382950782776\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4236689805984497\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41634446382522583\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4214426577091217\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4330839216709137\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43130815029144287\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 6050 | Loss: 0.4313 | Reward: 0.8047 | Value: 6.2163 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4394095242023468\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43023496866226196\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4306567311286926\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43085595965385437\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41843605041503906\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42178454995155334\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4217539131641388\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42803916335105896\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42561817169189453\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42738762497901917\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 6060 | Loss: 0.4274 | Reward: 0.8058 | Value: 6.2578 | GPU: 1186MB | Time/batch: 0.03s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41661134362220764\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41509121656417847\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4182038903236389\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4258173108100891\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42995455861091614\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4270993769168854\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4279167056083679\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42627304792404175\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4254705607891083\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42693638801574707\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 6070 | Loss: 0.4269 | Reward: 0.8049 | Value: 6.3208 | GPU: 1186MB | Time/batch: 0.04s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41611263155937195\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.426596462726593\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4405236840248108\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.427573025226593\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42050105333328247\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4115493595600128\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.434822678565979\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.40906351804733276\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42857450246810913\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4243059754371643\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 6080 | Loss: 0.4243 | Reward: 0.7986 | Value: 6.3247 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4305328130722046\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42102962732315063\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4197508990764618\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41320034861564636\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42764633893966675\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4341653287410736\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42399874329566956\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42225944995880127\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4323081970214844\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42310959100723267\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 6090 | Loss: 0.4231 | Reward: 0.7936 | Value: 6.3940 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43065255880355835\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4285227060317993\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42583394050598145\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41588887572288513\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.44220441579818726\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4224308431148529\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42770180106163025\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41981619596481323\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4187459349632263\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4264456033706665\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 6100 | Loss: 0.4264 | Reward: 0.7886 | Value: 6.4346 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4141296446323395\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42375147342681885\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4280330240726471\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42347341775894165\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4186689257621765\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42150193452835083\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4272952079772949\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42836007475852966\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4205537438392639\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4269907474517822\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 6110 | Loss: 0.4270 | Reward: 0.7873 | Value: 6.4971 | GPU: 1186MB | Time/batch: 0.03s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4277632236480713\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42186102271080017\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4243042767047882\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4316309690475464\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4195196032524109\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4229978322982788\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4274561405181885\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4279261529445648\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4283445179462433\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43076854944229126\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 6120 | Loss: 0.4308 | Reward: 0.7863 | Value: 6.5073 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4349413514137268\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4347088038921356\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4086560010910034\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4171028733253479\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4141552448272705\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4219036400318146\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4292566776275635\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4329988956451416\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4271467328071594\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4220930337905884\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 6130 | Loss: 0.4221 | Reward: 0.7855 | Value: 6.5356 | GPU: 1186MB | Time/batch: 0.03s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42099547386169434\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4235232472419739\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42008474469184875\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4291243851184845\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4299250543117523\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4129711389541626\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4252418875694275\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4238063395023346\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41261735558509827\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4236965477466583\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 6140 | Loss: 0.4237 | Reward: 0.7867 | Value: 6.5332 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4264296889305115\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42199134826660156\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42550888657569885\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41895365715026855\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4171164333820343\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4208712875843048\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4248286783695221\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42056789994239807\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4284729063510895\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42645955085754395\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 6150 | Loss: 0.4265 | Reward: 0.7894 | Value: 6.5586 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42037492990493774\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41203635931015015\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4318583607673645\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4185628294944763\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4184311330318451\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4290769100189209\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4253208339214325\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42383134365081787\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4292620122432709\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.427764356136322\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 6160 | Loss: 0.4278 | Reward: 0.7917 | Value: 6.5962 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4196161925792694\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42239177227020264\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4157889485359192\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.434520423412323\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4330214560031891\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41537243127822876\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4278136193752289\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42502158880233765\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43404656648635864\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42263466119766235\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 6170 | Loss: 0.4226 | Reward: 0.7874 | Value: 6.6709 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42368149757385254\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4221765995025635\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4363776743412018\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43434637784957886\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4242362082004547\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4294934570789337\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43278080224990845\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4300116300582886\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.424696147441864\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43472129106521606\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 6180 | Loss: 0.4347 | Reward: 0.7916 | Value: 6.7095 | GPU: 1186MB | Time/batch: 0.01s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43365925550460815\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41558516025543213\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.427329957485199\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43351325392723083\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42644989490509033\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4261280298233032\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4240295886993408\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41996800899505615\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4290964603424072\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4249676764011383\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 6190 | Loss: 0.4250 | Reward: 0.7953 | Value: 6.7061 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43436992168426514\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4348742663860321\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41945528984069824\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43208998441696167\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4282743036746979\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43241173028945923\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42954468727111816\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41679418087005615\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4275559186935425\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4280087947845459\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 6200 | Loss: 0.4280 | Reward: 0.7980 | Value: 6.7056 | GPU: 1186MB | Time/batch: 0.04s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42981216311454773\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4182146191596985\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4306544363498688\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.422880619764328\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4256155490875244\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41995084285736084\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42465537786483765\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4313986897468567\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4317721426486969\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43142327666282654\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 6210 | Loss: 0.4314 | Reward: 0.7966 | Value: 6.7329 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43121713399887085\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4256819784641266\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4298350512981415\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4218748211860657\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42229488492012024\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4283810555934906\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42866766452789307\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4372228980064392\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4250967800617218\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42912209033966064\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 6220 | Loss: 0.4291 | Reward: 0.7967 | Value: 6.7124 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4291779398918152\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4291708767414093\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.41784656047821045\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4222300052642822\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42704781889915466\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4262557625770569\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4354711174964905\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4293191432952881\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42056578397750854\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43052953481674194\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 6230 | Loss: 0.4305 | Reward: 0.7983 | Value: 6.6777 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43251436948776245\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4226948022842407\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43858736753463745\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4351389706134796\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42526406049728394\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4175099730491638\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4233325123786926\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42607152462005615\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4292519986629486\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.43121543526649475\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","Epoch 5 | Batch 6240 | Loss: 0.4312 | Reward: 0.7997 | Value: 6.6611 | GPU: 1186MB | Time/batch: 0.02s\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4330914616584778\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4271432161331177\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4355910122394562\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4326920509338379\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42567265033721924\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4320622682571411\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4310605227947235\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.4333585202693939\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","ðŸ” PPO Update Start\n","ðŸ”¹ Advantages: torch.Size([8, 1])\n","âœ… Log Probs: torch.Size([8]), âœ… Values: torch.Size([8])\n","ðŸ”„ Returns shape: torch.Size([8])\n","ðŸ“‰ Loss: 0.42669057846069336\n","âœ… PPO Update Done (Loss computed)! Backprop and Step outside.\n","âœ… Epoch 5 done in 124.68 seconds.\n","âœ… Model checkpoint saved: bhai_llm_epoch_5.pt\n"]}]},{"cell_type":"code","source":["# file ipython-input-19-f30f37f21b8b\n","import torch\n","from pathlib import Path\n","# from Tokenization.tokenization import TokenizationLayer\n","# from TokenEmbeddings.TokenEmbeddings import TokenEmbedding\n","# from Layer1.Layer1 import Layer1\n","# from Layer2WithMemory.Layer2WithMemory import Layer2WithMemory\n","# from Layer3WithContextAndRL.Layer3WithContextAndRL import Layer3WithContextAndRL\n","import torch.nn as nn  # Import nn for the projection layer\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","\n","# --- Load model and tokenizer ---\n","def load_model_and_tokenizer(pt_path):\n","    # Assuming TokenizationLayer is defined/imported from a previous cell\n","    tokenizer = TokenizationLayer()\n","\n","    embed_dim = 256\n","    hidden_dim = 1024\n","    num_heads = 8\n","    memory_size = 100\n","    # Add num_actions here as it's required by Layer3WithContextAndRL\n","    num_actions = 10  # Assuming this matches the training configuration\n","\n","    # Assuming TokenEmbedding, Layer1, Layer2WithMemory, Layer3WithContextAndRL\n","    # are defined/imported from previous cells\n","    embedder = TokenEmbedding(tokenizer.vocab_size, embed_dim=embed_dim).to(device)\n","    layer1 = Layer1(embed_dim, hidden_dim, num_heads).to(device)\n","    layer2 = Layer2WithMemory(embed_dim, hidden_dim, num_heads, memory_size).to(device)\n","    # Pass num_actions to Layer3 constructor\n","    layer3 = Layer3WithContextAndRL(embed_dim, hidden_dim, num_heads, memory_size, num_actions=num_actions).to(device)\n","\n","    checkpoint = torch.load(pt_path, map_location=device)\n","\n","    try:\n","        # Load state_dict for embedder, layer1, and layer2\n","        embedder.load_state_dict(checkpoint['embedder_state_dict'])\n","        layer1.load_state_dict(checkpoint['layer1_state_dict'])\n","        layer2.load_state_dict(checkpoint['layer2_state_dict'])\n","\n","        # Load Layer3 state_dict\n","        # Assuming the keys in the checkpoint match the Layer3 in ipython-input-10.\n","        layer3.load_state_dict(checkpoint['layer3_state_dict'])\n","\n","\n","    except Exception as e:  # Catch general exceptions during loading\n","        print(f\"ðŸš¨ Error loading state_dict: {e}. Ensure model architecture matches checkpoint.\")\n","        raise  # Re-raise the error after printing context\n","\n","    # Set all layers to evaluation mode\n","    embedder.eval()\n","    layer1.eval()\n","    layer2.eval()\n","    layer3.eval()\n","\n","    # Ensure memory_bank parameter in Layer3 is on the correct device if it exists\n","    # and not loaded by state_dict (though state_dict should handle this)\n","    if hasattr(layer3, 'memory_bank') and isinstance(layer3.memory_bank, torch.nn.Parameter):\n","        layer3.memory_bank.data = layer3.memory_bank.data.to(device)\n","        # Ensure memory gate weights are also on the correct device if not handled by state_dict\n","        if hasattr(layer3, 'memory_gate'):\n","            layer3.memory_gate.to(device)\n","\n","    # Return the loaded modules\n","    return tokenizer, embedder, layer1, layer2, layer3\n","\n","\n","# --- Greedy decoding function ---\n","@torch.no_grad()\n","def generate_text(tokenizer, embedder, layer1, layer2, layer3, prompt, max_length=50):\n","    # Use the same max_length as the trainer for consistency in input size for embeddings\n","    initial_input_length = 128  # Match trainer's max_seq_len\n","    input_ids = tokenizer.tokenize([prompt], max_length=initial_input_length).to(device)  # (1, initial_input_length)\n","\n","    # We will append generated tokens to input_ids, so start with the tokenized prompt\n","    generated_input_ids = input_ids  # Shape (1, current_seq_len)\n","\n","    # Ensure the model is on the correct device and in eval mode before generating\n","    embedder.to(device).eval()\n","    layer1.to(device).eval()\n","    layer2.to(device).eval()\n","    layer3.to(device).eval()\n","\n","    # Define a projection layer from embed_dim to vocab_size\n","    # This layer is NOT trained. Its weights are random or default initialized.\n","    # This is a workaround to get token logits from a model not trained for this.\n","    vocab_size = tokenizer.vocab_size\n","    embed_dim = layer3.embed_dim  # Get embed_dim from Layer3 instance\n","    token_projection_layer = nn.Linear(embed_dim, vocab_size).to(device)\n","    # Initialize the projection layer (optional, but might help slightly)\n","    nn.init.xavier_uniform_(token_projection_layer.weight)\n","    if token_projection_layer.bias is not None:\n","        nn.init.zeros_(token_projection_layer.bias)\n","\n","    # Use a loop for generation\n","    for _ in range(max_length):  # Generate up to max_length *new* tokens\n","        # For attention models, the forward pass usually takes the *entire* sequence so far.\n","        # If the sequence length exceeds the model's max_seq_len (128 in this case),\n","        # you need to handle this, e.g., by truncating the input to the last `initial_input_length` tokens.\n","        current_seq_len = generated_input_ids.shape[1]\n","        if current_seq_len > initial_input_length:\n","            # Truncate the input sequence to the model's expected max length\n","            model_input_ids = generated_input_ids[:,\n","                              -initial_input_length:]  # Take the last `initial_input_length` tokens\n","        else:\n","            model_input_ids = generated_input_ids  # Use the whole sequence if shorter than max length\n","\n","        # Apply embeddings, layer1, layer2, and layer3\n","        # Ensure inputs to layers are on the correct device\n","        model_input_ids = model_input_ids.to(device)\n","        with torch.no_grad():  # Ensure no gradients are computed during inference\n","            emb = embedder(model_input_ids)\n","            l1_out = layer1(emb)\n","            l2_out = layer2(l1_out)\n","            # Layer3 forward returns 5 values. `out` is the hidden states of the last layer.\n","            out, _, _, _, _ = layer3(l2_out, actions=None)  # Pass actions=None for inference\n","\n","        # Get the hidden state for the *last* token in the output sequence\n","        # `out` has shape (batch_size, current_model_seq_len, embed_dim)\n","        last_hidden_state = out[:, -1, :]  # Shape (1, embed_dim)\n","\n","        # Project the last hidden state to the vocabulary size to get token logits\n","        # Ensure the hidden state is on the correct device for the projection layer\n","        next_token_logits = token_projection_layer(last_hidden_state.to(device))  # Shape (1, vocab_size)\n","\n","        # Select the token with the highest probability (greedy decoding)\n","        # Ensure next_token_logits is on the correct device\n","        next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(1)  # Shape (1, 1)\n","\n","        # Append the predicted token to the generated sequence\n","        generated_input_ids = torch.cat((generated_input_ids, next_token_id), dim=1)  # Shape (1, current_seq_len + 1)\n","\n","        # Stop if EOS token is generated\n","        # Check if the generated token is the EOS token ID\n","        if next_token_id.item() == tokenizer.eos_token_id:\n","            break\n","\n","    # Remove the prompt tokens from the generated sequence to get the response tokens\n","    # Ensure generated_input_ids is on CPU and converted to list for slicing\n","    generated_tokens_list = generated_input_ids[0].cpu().tolist()\n","    input_ids_list = input_ids[0].cpu().tolist()\n","\n","    # Find the index of the first token after the initial prompt.\n","    # This assumes the generated_input_ids starts exactly with input_ids.\n","    # If the prompt itself contains the EOS token, this might behave unexpectedly.\n","    # A more robust approach might be to just take the tokens appended *after* the loop starts.\n","    # However, sticking to the original slicing logic:\n","    # Ensure the slicing is valid\n","    if len(generated_tokens_list) > len(input_ids_list):\n","        response_tokens = generated_tokens_list[len(input_ids_list):]\n","    else:\n","        # If no new tokens were generated beyond the prompt length\n","        response_tokens = []\n","        # print(\"Warning: No tokens generated beyond the prompt length.\")\n","\n","    # Detokenize the response tokens using the *correct* method name\n","    generated_text = tokenizer.detokenize(response_tokens)  # âœ… Correct method name\n","    return generated_text\n","\n","\n","# --- Main chatbot loop ---\n","if __name__ == \"__main__\":\n","    # Define pt_path here\n","    pt_path = \"/content/bhai_llm_epoch_5.pt\"  # âœ… Change to your checkpoint path\n","\n","    if not Path(pt_path).exists():\n","        print(f\"ðŸš¨ Checkpoint not found: {pt_path}\")\n","        exit(1)\n","\n","    try:\n","        tokenizer, embedder, layer1, layer2, layer3 = load_model_and_tokenizer(pt_path)\n","        print(\"âœ… Model and tokenizer loaded!\")\n","        print(\"ðŸ¤– Chatbot ready. Type 'quit' to exit.\")\n","\n","        while True:\n","            user_input = input(\"You: \")\n","            if user_input.lower() in ['quit', 'exit']:\n","                print(\"Bye! ðŸ‘‹\")\n","                break\n","\n","            # Ensure input is on the correct device before passing to generate_text if needed,\n","            # but generate_text handles moving tensors to device.\n","            try:\n","                response = generate_text(tokenizer, embedder, layer1, layer2, layer3, user_input, max_length=50)\n","                print(f\"Bot: {response}\")\n","            except Exception as gen_error:\n","                print(f\"ðŸš¨ An error occurred during text generation: {gen_error}\")\n","                # You might want to add more specific error handling ocr debug prints here.\n","\n","\n","    except Exception as load_error:  # Catch errors during model loading\n","        print(f\"An error occurred during model loading: {load_error}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":758},"id":"gkhfhmtCTftW","executionInfo":{"status":"error","timestamp":1749755037699,"user_tz":-330,"elapsed":252965,"user":{"displayName":"Ankit kashyap","userId":"13344809073766364463"}},"outputId":"ce7825db-7453-433d-f996-0a7dbfc530c0"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","âœ… Model and tokenizer loaded!\n","ðŸ¤– Chatbot ready. Type 'quit' to exit.\n","You: hi\n","Bot: wswswswswswswswswswswswswswswswswswswswswswswswswswswswswswswswswswswswswswswswswswswswswswswswswsws\n","You: hi\n","Bot:  profitable profitable profitable profitable profitable profitable profitable profitable profitable profitable profitable profitable profitable profitable profitable profitable profitable profitable profitable profitable profitable profitable profitable profitable profitable profitable profitable profitable profitable profitable profitable profitable profitable profitable profitable profitable profitable profitable profitable profitable profitable profitable profitable profitable profitable profitable profitable profitable profitable profitable\n","You: hello\n","Bot: egisegisegisegisegisegisegisegisegisegisegisegisegisegisegisegisegisegisegisegisegisegisegisegisegisegisegisegisegisegisegisegisegisegisegisegisegisegisegisegisegisegisegisegisegisegisegisegisegisegis\n","You: tell me something\n","Bot: (cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel(cancel\n","You: how are you\n","Bot: ')==')==')==')==')==')==')==')==')==')==')==')==')==')==')==')==')==')==')==')==')==')==')==')==')==')==')==')==')==')==')==')==')==')==')==')==')==')==')==')==')==')==')==')==')==')==')==')==')==')==\n","You: say\n","Bot: imusimusimusimusimusimusimusimusimusimusimusimusimusimusimusimusimusimusimusimusimusimusimusimusimusimusimusimusimusimusimusimusimusimusimusimusimusimusimusimusimusimusimusimusimusimusimusimusimusimus\n","You: fly'\n","Bot: treetreetreetreetreetreetreetreetreetreetreetreetreetreetreetreetreetreetreetreetreetreetreetreetreetreetreetreetreetreetreetreetreetreetreetreetreetreetreetreetreetreetreetreetreetreetreetreetreetree\n","You: can you sing?\n","Bot:  Hours Hours Hours Hours Hours Hours Hours Hours Hours Hours Hours Hours Hours Hours Hours Hours Hours Hours Hours Hours Hours Hours Hours Hours Hours Hours Hours Hours Hours Hours Hours Hours Hours Hours Hours Hours Hours Hours Hours Hours Hours Hours Hours Hours Hours Hours Hours Hours Hours Hours\n","You: what is an AI\n","Bot:  CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef CrossRef\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"Interrupted by user","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-39-1934918314>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'quit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Bye! ðŸ‘‹\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}]}]}